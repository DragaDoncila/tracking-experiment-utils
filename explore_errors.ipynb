{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yaml\n",
    "SOL_ROOT = '/home/ddon0001/PhD/experiments/scaled/no_div_constraint_err_seg'\n",
    "NO_MERGE_ROOT = '/home/ddon0001/PhD/experiments/scaled/no_merges_all'\n",
    "SCALES_PATH = '/home/ddon0001/PhD/data/cell_tracking_challenge/scales.yaml'\n",
    "\n",
    "ds_summary = pd.read_csv(f'{SOL_ROOT}/summary.csv')\n",
    "with open(SCALES_PATH, 'r') as f:\n",
    "    scales = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EDGE_FN = 'EdgeFlag.FALSE_NEG'\n",
    "EDGE_FP = 'EdgeFlag.FALSE_POS'\n",
    "EDGE_WS = 'EdgeFlag.WRONG_SEMANTIC'\n",
    "\n",
    "NODE_FN = 'NodeFlag.FALSE_NEG'\n",
    "NODE_FP = 'NodeFlag.FALSE_POS'\n",
    "NODE_NS = 'NodeFlag.NON_SPLIT'\n",
    "\n",
    "def has_adjacent_error_edges(sol, node):\n",
    "    if any(sol[edge][EDGE_FP] or sol[edge][EDGE_WS] for edge in sol.in_edges(node)):\n",
    "        return True\n",
    "    if any(sol[edge][EDGE_FP] or sol[edge][EDGE_WS] for edge in sol.out_edges(node)):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import json\n",
    "import os\n",
    "import networkx as nx\n",
    "\n",
    "total_fn = defaultdict(lambda : 0)\n",
    "total_edges = defaultdict(lambda : 0)\n",
    "all_edge_count = defaultdict(lambda : 0)\n",
    "ds_names = []\n",
    "us = []\n",
    "gt_us = []\n",
    "merge_vs = []\n",
    "true_successors = []\n",
    "correct_fate = []\n",
    "is_immediate_split = []\n",
    "\n",
    "count_should_divide = defaultdict(lambda : 0)\n",
    "for _, row in ds_summary.iterrows():\n",
    "    ds_name = row['ds_name']\n",
    "    gt_solution_path = os.path.join(SOL_ROOT, ds_name, 'matched_gt.graphml')\n",
    "    no_merge_gt_pth = os.path.join(NO_MERGE_ROOT, ds_name, 'matched_gt.graphml')\n",
    "    solution_path = os.path.join(SOL_ROOT, ds_name, 'matched_solution.graphml')\n",
    "    no_merge_sol_pth = os.path.join(NO_MERGE_ROOT, ds_name, 'matched_solution.graphml')\n",
    "    node_match_path = os.path.join(SOL_ROOT, ds_name, 'matching.json')\n",
    "    \n",
    "    \n",
    "    merge_gt = nx.read_graphml(gt_solution_path)\n",
    "    no_merge_gt = nx.read_graphml(no_merge_gt_pth)\n",
    "    merge_g = nx.read_graphml(solution_path, node_type=int)\n",
    "    no_merge_g = nx.read_graphml(no_merge_sol_pth, node_type=int)\n",
    "    with open(node_match_path) as f:\n",
    "        node_match = json.load(f)\n",
    "    gt_to_sol = {item[0]: item[1] for item in node_match}\n",
    "    sol_to_gt = {item[1]: item[0] for item in node_match}\n",
    "\n",
    "    total_fn[ds_name] = len([node for node in merge_gt.nodes if merge_gt.nodes[node][NODE_FN]])\n",
    "    all_edge_count[ds_name] = merge_gt.number_of_edges()\n",
    "    for merge_node in merge_g.nodes:\n",
    "        # we have a merge vertex\n",
    "        if merge_g.in_degree(merge_node) > 1:\n",
    "            parents = merge_g.predecessors(merge_node)\n",
    "            node_gt = sol_to_gt[merge_node]\n",
    "            for parent in parents:\n",
    "                # # should this parent be dividing?\n",
    "                # gt_parent = sol_to_gt[parent]\n",
    "                # if merge_gt.out_degree(gt_parent) > 1:\n",
    "                #     assert merge_g.out_degree(parent) <= 1\n",
    "                #     count_should_divide[ds_name] += 1\n",
    "                # continue\n",
    "                edge_of_interest = (parent, merge_node)\n",
    "                # this edge doesn't exist in gt\n",
    "                if merge_g.edges[edge_of_interest][EDGE_FP]:\n",
    "                    ds_names.append(ds_name)\n",
    "                    us.append(parent)\n",
    "                    merge_vs.append(merge_node)\n",
    "                    total_edges[ds_name] += 1\n",
    "                    if merge_g.out_degree(merge_node) > 1:\n",
    "                        is_immediate_split.append(True)\n",
    "                    else:\n",
    "                        is_immediate_split.append(False)\n",
    "                    \n",
    "                    # so what does gt_parent usually do?\n",
    "                    gt_parent = sol_to_gt[parent]\n",
    "                    gt_us.append(gt_parent)\n",
    "                    gt_kids = list(merge_gt.successors(gt_parent))\n",
    "                    n_kids = len(gt_kids)\n",
    "                    if n_kids == 0:\n",
    "                        # \"parent\" should be terminating\n",
    "                        true_successors.append('-4')\n",
    "                        correct_fate.append('terminate')\n",
    "                        continue\n",
    "                    elif n_kids == 1:\n",
    "                        kid = gt_kids[0]\n",
    "                        if merge_gt.nodes[kid]['t'] > merge_gt.nodes[gt_parent]['t'] + 1:\n",
    "                            # \"parent\" is frame skipping\n",
    "                            true_successors.append(kid)\n",
    "                            correct_fate.append('skip')\n",
    "                            continue\n",
    "                        else:\n",
    "                            # we're missing this vertex\n",
    "                            if merge_gt.nodes[kid][NODE_FN]:\n",
    "                                true_successors.append(kid)\n",
    "                                correct_fate.append('introduce')\n",
    "                                continue\n",
    "                            else:\n",
    "                                # \"parent\" should be migrating normally elsewhere\n",
    "                                true_successors.append(kid)\n",
    "                                correct_fate.append('migrate')\n",
    "                                continue\n",
    "                    raise ValueError(f'Parent {parent} fate missing!')\n",
    "                    #     # parent is splitting into two totally different children\n",
    "                    #     should_divide[ds_name].append(parent)\n",
    "merge_fate_df = pd.DataFrame({\n",
    "    'ds_name': ds_names,\n",
    "    'u': us,\n",
    "    'gt_u': gt_us,\n",
    "    'v': merge_vs,\n",
    "    'gt_vs': true_successors,\n",
    "    'correct_fate': correct_fate,\n",
    "    'is_immediate_split': is_immediate_split\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_fate_df['new_fate'] = 'Unknown'\n",
    "merge_fate_df['new_v'] = -1\n",
    "merge_fate_df['new_v_2'] = -1\n",
    "merge_fate_df['new_fate_correct'] = False\n",
    "for ds_name in merge_fate_df['ds_name'].unique():\n",
    "    original_merge_fps = merge_fate_df[merge_fate_df['ds_name'] == ds_name]\n",
    "\n",
    "    solution_path = os.path.join(SOL_ROOT, ds_name, 'matched_solution.graphml')\n",
    "    no_merge_sol_pth = os.path.join(NO_MERGE_ROOT, ds_name, 'matched_solution.graphml')\n",
    "    node_match_path = os.path.join(NO_MERGE_ROOT, ds_name, 'matching.json')\n",
    "    gt_solution_path = os.path.join(NO_MERGE_ROOT, ds_name, 'matched_gt.graphml')\n",
    "    \n",
    "    merge_g = nx.read_graphml(solution_path, node_type=int)\n",
    "    no_merge_g = nx.read_graphml(no_merge_sol_pth, node_type=int)\n",
    "    gt_g = nx.read_graphml(gt_solution_path)\n",
    "\n",
    "    with open(node_match_path) as f:\n",
    "        node_match = json.load(f)\n",
    "    gt_to_sol = {item[0]: item[1] for item in node_match}\n",
    "    sol_to_gt = {item[1]: item[0] for item in node_match}\n",
    "    for row in original_merge_fps.itertuples():\n",
    "        u = row.u\n",
    "        original_successors = list(merge_g.successors(u))\n",
    "        new_successors = list(no_merge_g.successors(u))\n",
    "        if len(new_successors) == 0:\n",
    "            merge_fate_df.loc[row.Index, 'new_fate'] = 'terminate'\n",
    "            merge_fate_df.loc[row.Index, 'new_v'] = -4\n",
    "            if row.correct_fate == 'terminate':\n",
    "                merge_fate_df.loc[row.Index, 'new_fate_correct'] = True\n",
    "        elif len(new_successors) == 1:\n",
    "            merge_fate_df.loc[row.Index, 'new_fate'] = 'migrate'\n",
    "            merge_fate_df.loc[row.Index, 'new_v'] = new_successors[0]\n",
    "            if row.correct_fate == 'migrate' and row.gt_vs == sol_to_gt[new_successors[0]]:\n",
    "                merge_fate_df.loc[row.Index, 'new_fate_correct'] = True\n",
    "        elif len(new_successors) == 2:\n",
    "            shared_successors = set(original_successors).intersection(set(new_successors))\n",
    "            if len(shared_successors) == 2:\n",
    "                raise ValueError(f'{ds_name} vertex {u} still has the same two successors.')\n",
    "            elif len(shared_successors) == 1:\n",
    "                # we share one successor and swapped the other one.\n",
    "                # is the shared successor the merge vertex?\n",
    "                if shared_successors.pop() == row.v:\n",
    "                    raise(ValueError(f'{ds_name} vertex {u} still flowing into merge vertex.'))\n",
    "                other_successor = list(set(original_successors) - shared_successors)[0]\n",
    "                merge_fate_df.loc[row.Index, 'new_fate'] = 'migrate'\n",
    "                merge_fate_df.loc[row.Index, 'new_v'] = other_successor\n",
    "            elif len(shared_successors) == 0:\n",
    "                # we weren't splitting before but now we are\n",
    "                if len(original_successors) == 1:\n",
    "                    merge_fate_df.loc[row.Index, 'new_fate'] = 'new_split'\n",
    "                # we were always splitting but now it's different\n",
    "                else:\n",
    "                    merge_fate_df.loc[row.Index, 'new_fate'] = 'different_split'\n",
    "                merge_fate_df.loc[row.Index, 'new_v'] = new_successors[0]\n",
    "                merge_fate_df.loc[row.Index, 'new_v_2'] = new_successors[1]\n",
    "        else:\n",
    "            raise ValueError(f'{ds_name} unexpected number of successors for vertex {u}: {len(new_successors)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "merge_fate_df['old_cost'] = -1.0\n",
    "merge_fate_df['new_cost'] = -1.0\n",
    "merge_fate_df['old_area_prop'] = -1.0\n",
    "merge_fate_df['new_area_prop'] = -1.0\n",
    "merge_fate_df['old_chosen_neighbour_rank'] = -1\n",
    "merge_fate_df['new_chosen_neighbour_rank'] = -1\n",
    "merge_fate_df['old_sensitivity_diff'] = -1.0\n",
    "merge_fate_df['new_sensitivity_diff'] = -1.0\n",
    "\n",
    "# load merge edges with target and non merge edges with target\n",
    "all_edges_with_target_merge = pd.read_csv(f'{SOL_ROOT}/all_edges_with_target.csv')\n",
    "all_edges_with_target_no_merge = pd.read_csv(f'{NO_MERGE_ROOT}/all_edges_with_target.csv')\n",
    "for edge_row in merge_fate_df.itertuples():\n",
    "    # for each edge, assign the values of their features\n",
    "    merge_edge = all_edges_with_target_merge[(all_edges_with_target_merge.ds_name == edge_row.ds_name) & (all_edges_with_target_merge.u == edge_row.u) & (all_edges_with_target_merge.v == edge_row.v)]\n",
    "    if len(merge_edge) == 0:\n",
    "        print(\"MISSING MERGE EDGE\")\n",
    "    # assign into df\n",
    "    merge_fate_df.loc[edge_row.Index, 'old_cost'] = merge_edge.cost.values[0]\n",
    "    merge_fate_df.loc[edge_row.Index, 'old_area_prop'] = merge_edge.chosen_neighbour_area_prop.values[0]\n",
    "    merge_fate_df.loc[edge_row.Index, 'old_chosen_neighbour_rank'] = merge_edge.chosen_neighbour_rank.values[0]\n",
    "    merge_fate_df.loc[edge_row.Index, 'old_sensitivity_diff'] = merge_edge.sensitivity_diff.values[0]\n",
    "\n",
    "    if edge_row.new_v == -4:\n",
    "        # need to read the actual \"all edges file for this dataset\"\n",
    "        ds_all_edges = pd.read_csv(f'{SOL_ROOT}/{edge_row.ds_name}/all_edges.csv')\n",
    "        non_merge_edge = ds_all_edges[(ds_all_edges.u == edge_row.u) & (ds_all_edges.v == edge_row.new_v)]\n",
    "        merge_fate_df.loc[edge_row.Index, 'new_cost'] = non_merge_edge.cost.values[0]\n",
    "        merge_fate_df.loc[edge_row.Index, 'new_area_prop'] = 0\n",
    "        merge_fate_df.loc[edge_row.Index, 'new_sensitivity_diff'] = non_merge_edge.sensitivity_diff.values[0]\n",
    "        # need to rank the neighbour\n",
    "        all_neighbours_of_u_costs = ds_all_edges[(ds_all_edges.u == edge_row.u) & (ds_all_edges.v >= 0)].cost.values\n",
    "        all_neighbours_of_u_costs = np.append(all_neighbours_of_u_costs, non_merge_edge.cost.values[0])\n",
    "        merge_fate_df.loc[edge_row.Index, 'new_chosen_neighbour_rank'] = all_neighbours_of_u_costs.argsort().argsort()[-1]\n",
    "        continue \n",
    "\n",
    "    non_merge_edge = all_edges_with_target_no_merge[(all_edges_with_target_no_merge.ds_name == edge_row.ds_name) & (all_edges_with_target_no_merge.u == edge_row.u) & (all_edges_with_target_no_merge.v == edge_row.new_v)]\n",
    "    if len(non_merge_edge) == 0:\n",
    "        print(\"MISSING NON MERGE EDGE\")\n",
    "    merge_fate_df.loc[edge_row.Index, 'new_cost'] = non_merge_edge.cost.values[0]\n",
    "    merge_fate_df.loc[edge_row.Index, 'new_area_prop'] = non_merge_edge.chosen_neighbour_area_prop.values[0]\n",
    "    merge_fate_df.loc[edge_row.Index, 'new_chosen_neighbour_rank'] = non_merge_edge.chosen_neighbour_rank.values[0]\n",
    "    merge_fate_df.loc[edge_row.Index, 'new_sensitivity_diff'] = non_merge_edge.sensitivity_diff.values[0]\n",
    "merge_fate_df.to_csv('/home/ddon0001/PhD/experiments/merge_resolution/merge_fates.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "merge_fate_df = pd.read_csv('/home/ddon0001/PhD/experiments/merge_resolution/merge_fates.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_fate_df[merge_fate_df.ds_name == 'BF-C2DL-MuSC_01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_fate_df.groupby('correct_fate').new_fate.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_fate_df.groupby('is_immediate_split').correct_fate.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_fate_df.is_immediate_split.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "plot_order = merge_fate_df.ds_name.value_counts().index\n",
    "merge_fate_df['chosen_rank_diff'] = merge_fate_df.new_chosen_neighbour_rank - merge_fate_df.old_chosen_neighbour_rank\n",
    "merge_fate_df['sensitivity_diff_diff'] = merge_fate_df.new_sensitivity_diff - merge_fate_df.old_sensitivity_diff\n",
    "merge_fate_df['area_prop_diff'] = merge_fate_df.new_area_prop - merge_fate_df.old_area_prop\n",
    "merge_fate_df['cost_diff'] = merge_fate_df.new_cost - merge_fate_df.old_cost\n",
    "\n",
    "grid = sns.FacetGrid(merge_fate_df, col='ds_name', col_order=plot_order, col_wrap=4, hue='correct_fate', sharex=False, sharey=False)\n",
    "# grid.map(sns.scatterplot, 'old_cost', 'new_cost')\n",
    "# grid.map(sns.scatterplot, 'old_area_prop', 'new_area_prop')\n",
    "# grid.map(sns.scatterplot, 'old_chosen_neighbour_rank', 'new_chosen_neighbour_rank')\n",
    "grid.map(sns.scatterplot, 'old_sensitivity_diff', 'new_sensitivity_diff')\n",
    "\n",
    "# grid.map(sns.countplot, 'chosen_rank_diff')\n",
    "# grid.map(sns.histplot, 'cost_diff')\n",
    "# grid.map(sns.histplot, 'area_prop_diff')\n",
    "# grid.map(sns.histplot, 'sensitivity_diff_diff')\n",
    "grid.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge_fate_df[merge_fate_df.chosen_rank_diff == 0].correct_fate.value_counts()\n",
    "merge_fate_df[merge_fate_df.v == merge_fate_df.new_v].correct_fate.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many skip frames have we \"correctly\" captured through merge and split?\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import os\n",
    "import networkx as nx\n",
    "\n",
    "ds_names = []\n",
    "count_skip = []\n",
    "count_fn_adjacent = []\n",
    "count_correctly_merged = []\n",
    "for _, row in ds_summary.iterrows():\n",
    "    ds_name = row['ds_name']\n",
    "    gt_solution_path = os.path.join(SOL_ROOT, ds_name, 'matched_gt.graphml')\n",
    "    solution_path = os.path.join(SOL_ROOT, ds_name, 'matched_solution.graphml')\n",
    "    node_match_path = os.path.join(SOL_ROOT, ds_name, 'matching.json')\n",
    "    \n",
    "    merge_gt = nx.read_graphml(gt_solution_path)\n",
    "    merge_g = nx.read_graphml(solution_path, node_type=int)\n",
    "\n",
    "    with open(node_match_path) as f:\n",
    "        node_match = json.load(f)\n",
    "    gt_to_sol = {item[0]: item[1] for item in node_match}\n",
    "    sol_to_gt = {item[1]: item[0] for item in node_match}\n",
    "\n",
    "    skip_edges = [edge for edge in merge_gt.edges if merge_gt.nodes[edge[0]]['t'] < merge_gt.nodes[edge[1]]['t'] - 1]\n",
    "    count_correct = 0\n",
    "    count_fn = 0\n",
    "    for edge in skip_edges:\n",
    "        if edge[0] not in gt_to_sol or edge[1] not in gt_to_sol:\n",
    "            count_fn += 1\n",
    "            continue\n",
    "        sol_parent = gt_to_sol[edge[0]]\n",
    "        sol_child = gt_to_sol[edge[1]]\n",
    "        # we want parent to be flowing into a merge\n",
    "        if merge_g.out_degree(sol_parent) == 1:\n",
    "            successor = list(merge_g.successors(sol_parent))[0]\n",
    "            if merge_g.in_degree(successor) > 1:\n",
    "                # sol_parent is flowing into a merge - follow the merge to its end\n",
    "                v = successor\n",
    "                children = list(merge_g.successors(v))\n",
    "                while len(children) == 1:\n",
    "                    v = children[0]\n",
    "                    children = list(merge_g.successors(v))\n",
    "                # we've either split or we've terminated\n",
    "                if len(children) > 1:\n",
    "                    # we've split - are any of the children sol_child?\n",
    "                    if any(child == sol_child for child in children):\n",
    "                        count_correct += 1\n",
    "    ds_names.append(ds_name)\n",
    "    count_correctly_merged.append(count_correct)\n",
    "    count_skip.append(len(skip_edges))\n",
    "    count_fn_adjacent.append(count_fn)\n",
    "skip_merges = pd.DataFrame({\n",
    "    'ds_name': ds_names,\n",
    "    'total_skips': count_skip,\n",
    "    'fn_adjacent': count_fn_adjacent,\n",
    "    'total_correct': count_correctly_merged\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_merges.to_csv(os.path.join(SOL_ROOT, 'skip_merges.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how long do merges continue for\n",
    "\n",
    "ds_names = []\n",
    "merge_id = []\n",
    "length_of_merge = []\n",
    "for _, row in ds_summary.iterrows():\n",
    "    ds_name = row['ds_name']\n",
    "    solution_path = os.path.join(SOL_ROOT, ds_name, 'matched_solution.graphml')\n",
    "    merge_g = nx.read_graphml(solution_path, node_type=int)\n",
    "\n",
    "    merges = [node for node in merge_g.nodes if merge_g.in_degree(node) > 1]\n",
    "\n",
    "    for merge_node in merges:\n",
    "        current_length_of_merge = 0\n",
    "        children = list(merge_g.successors(merge_node))\n",
    "        v = merge_node\n",
    "        while len(children) == 1:\n",
    "            v = children[0]\n",
    "            current_length_of_merge += 1\n",
    "            children = list(merge_g.successors(v))\n",
    "        ds_names.append(ds_name)\n",
    "        merge_id.append(merge_node)\n",
    "        length_of_merge.append(current_length_of_merge)\n",
    "length_df = pd.DataFrame({\n",
    "    'ds_name': ds_names,\n",
    "    'merge_id': merge_id,\n",
    "    'length': length_of_merge\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "plot_order = length_df.ds_name.value_counts().index\n",
    "grid = sns.FacetGrid(length_df, col='ds_name', col_wrap=4, sharex=False, sharey=False, col_order=plot_order)\n",
    "grid.map_dataframe(sns.histplot, 'length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_df.to_csv('/home/ddon0001/PhD/experiments/merge_resolution/merge_lengths.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_fate_df = merge_fate_df.merge(length_df, left_on=['ds_name', 'v'], right_on=['ds_name', 'merge_id'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(merge_fate_df[merge_fate_df.correct_fate == 'introduce'].length <= 2).sum()\n",
    "len(merge_fate_df[merge_fate_df.correct_fate == 'introduce'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_fate_df.to_csv('/home/ddon0001/PhD/experiments/merge_resolution/merge_fates.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many merges correctly fuel divisions?\n",
    "\n",
    "ds_names = []\n",
    "total_merges = []\n",
    "# looking at the fate of the merge now\n",
    "count_correct_division = []\n",
    "count_incorrect_division = []\n",
    "count_terminating_correct = []\n",
    "count_terminating_incorrect = []\n",
    "total_explained = []\n",
    "length_of_merge = []\n",
    "for _, row in ds_summary.iterrows():\n",
    "    ds_name = row['ds_name']\n",
    "    gt_solution_path = os.path.join(SOL_ROOT, ds_name, 'matched_gt.graphml')\n",
    "    solution_path = os.path.join(SOL_ROOT, ds_name, 'matched_solution.graphml')\n",
    "    node_match_path = os.path.join(SOL_ROOT, ds_name, 'matching.json')\n",
    "    \n",
    "    merge_gt = nx.read_graphml(gt_solution_path)\n",
    "    merge_g = nx.read_graphml(solution_path, node_type=int)\n",
    "\n",
    "    with open(node_match_path) as f:\n",
    "        node_match = json.load(f)\n",
    "    gt_to_sol = {item[0]: item[1] for item in node_match}\n",
    "    sol_to_gt = {item[1]: item[0] for item in node_match}\n",
    "\n",
    "    merges = [node for node in merge_g.nodes if merge_g.in_degree(node) > 1]\n",
    "    total = len(merges)\n",
    "    n_correct = 0\n",
    "    n_incorrect = 0\n",
    "    n_terminating_correct = 0\n",
    "    n_terminating_incorrect = 0\n",
    "    current_length_of_merge = 0\n",
    "\n",
    "    for merge_node in merges:\n",
    "        children = list(merge_g.successors(merge_node))\n",
    "        v = merge_node\n",
    "        while len(children) == 1:\n",
    "            v = children[0]\n",
    "            children = list(merge_g.successors(v))\n",
    "            current_length_of_merge += 1\n",
    "        if len(children) == 0:\n",
    "            gt_term = sol_to_gt[v]\n",
    "            # gt node also terminates\n",
    "            if len(list(merge_gt.successors(gt_term))) == 0:\n",
    "                n_terminating_correct += 1\n",
    "            else:\n",
    "                n_terminating_incorrect += 1\n",
    "        elif len(children) > 1:\n",
    "            div_edges = [(v, child) for child in children]\n",
    "            # are any edges wrong?\n",
    "            if any(merge_g.edges[edge][EDGE_FP] or merge_g.edges[edge][EDGE_WS] for edge in div_edges):\n",
    "                n_incorrect += 1\n",
    "            else:\n",
    "                n_correct += 1\n",
    "    ds_names.append(ds_name)\n",
    "    total_merges.append(total)\n",
    "    count_correct_division.append(n_correct)\n",
    "    count_incorrect_division.append(n_incorrect)\n",
    "    count_terminating_correct.append(n_terminating_correct)\n",
    "    count_terminating_incorrect.append(n_terminating_incorrect)\n",
    "    length_of_merge.append(current_length_of_merge)\n",
    "    total_explained.append(n_correct + n_incorrect + n_terminating_correct + n_terminating_incorrect)\n",
    "merge_fates = pd.DataFrame({\n",
    "    'ds_name': ds_names,\n",
    "    'total_merges': total_merges,\n",
    "    'total_explained': total_explained,\n",
    "    'correct_division': count_correct_division,\n",
    "    'incorrect_division': count_incorrect_division,\n",
    "    'terminating_correct': count_terminating_correct,\n",
    "    'terminating_incorrect': count_terminating_incorrect,\n",
    "    'length_of_merge': length_of_merge\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_fates[merge_fates['total_merges'] > 0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# symmetry/asymmetry of divisions \n",
    "import numpy as np\n",
    "\n",
    "\n",
    "div_diff_dict = {\n",
    "    'ds_name': [],\n",
    "    'div_parent_index': [],\n",
    "    'is_correct': [],\n",
    "    'is_superparent': [],\n",
    "    'child_distance_1': [],\n",
    "    'child_distance_2': [],\n",
    "    'child_distance_difference': [],\n",
    "    'interchild_distance': [],\n",
    "    'div_angle': []\n",
    "}\n",
    "\n",
    "# WE TAKE TWO CLOSEST CHILDREN\n",
    "for _, row in ds_summary.iterrows():\n",
    "    ds_name = row['ds_name']\n",
    "    solution_path = os.path.join(SOL_ROOT, ds_name, 'matched_solution.graphml')\n",
    "    \n",
    "    merge_g = nx.read_graphml(solution_path, node_type=int)\n",
    "    location_cols = ['z', 'y', 'x'] if 'z' in merge_g.nodes[0] else ['y', 'x']\n",
    "    scale = scales[ds_name]['pixel_scale']\n",
    "\n",
    "    divs = [node for node in merge_g.nodes if merge_g.out_degree(node) > 1]\n",
    "    for div in divs:\n",
    "        children = list(merge_g.successors(div))\n",
    "        is_superparent = len(children) > 2\n",
    "        is_correct = not any(merge_g.edges[(div, child)][EDGE_FP] or merge_g.edges[(div, child)][EDGE_WS] for child in children)\n",
    "        parent_coords = np.asarray([merge_g.nodes[div][col] * scale[i] for i, col in enumerate(location_cols)])\n",
    "        child_coords = [np.asarray([merge_g.nodes[child][col] * scale[i] for i, col in enumerate(location_cols)]) for child in children]\n",
    "        child_distances = [np.linalg.norm(parent_coords - child) for child in child_coords]\n",
    "        dist_indices = np.argsort(child_distances)\n",
    "        closest_children = [child_coords[i] for i in dist_indices[:2]]\n",
    "        distances = [child_distances[i] for i in dist_indices[:2]]\n",
    "        div_angle = np.degrees(np.arccos(\n",
    "            np.dot(closest_children[0] - parent_coords, closest_children[1] - parent_coords) /\n",
    "            (distances[0] * distances[1])\n",
    "        ))\n",
    "    \n",
    "        div_diff_dict['ds_name'].append(ds_name)\n",
    "        div_diff_dict['div_parent_index'].append(div)\n",
    "        div_diff_dict['is_correct'].append(is_correct)\n",
    "        div_diff_dict['is_superparent'].append(is_superparent)\n",
    "        div_diff_dict['child_distance_1'].append(distances[0])\n",
    "        div_diff_dict['child_distance_2'].append(distances[1])\n",
    "        div_diff_dict['child_distance_difference'].append(distances[1] - distances[0])\n",
    "        div_diff_dict['interchild_distance'].append(np.linalg.norm(closest_children[0] - closest_children[1]))\n",
    "        div_diff_dict['div_angle'].append(div_angle)\n",
    "\n",
    "div_diff_df = pd.DataFrame(div_diff_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "grid = sns.FacetGrid(div_diff_df, col='ds_name', col_wrap=4, sharex=False, sharey=False, legend_out=True, hue='is_correct')\n",
    "grid.map_dataframe(sns.histplot, x='interchild_distance')\n",
    "grid.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "grid = sns.FacetGrid(div_diff_df, col='ds_name', col_wrap=4, sharex=False, sharey=False, legend_out=True, hue='is_correct')\n",
    "grid.map_dataframe(sns.histplot, x='child_distance_difference')\n",
    "grid.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "grid = sns.FacetGrid(div_diff_df, col='ds_name', col_wrap=4, sharex=False, sharey=False, legend_out=True, hue='is_correct')\n",
    "grid.map_dataframe(sns.histplot, x='div_angle')\n",
    "grid.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "grid = sns.FacetGrid(div_diff_df, col='ds_name', col_wrap=4, sharex=True, sharey=False, legend_out=True, hue='is_correct')\n",
    "grid.map_dataframe(sns.scatterplot, x='div_angle', y='interchild_distance', alpha=0.4)\n",
    "grid.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "grid = sns.FacetGrid(div_diff_df, col='ds_name', col_wrap=4, sharex=True, sharey=False, legend_out=True, hue='is_correct')\n",
    "grid.map_dataframe(sns.scatterplot, x='div_angle', y='child_distance_difference', alpha=0.4)\n",
    "grid.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import networkx as nx\n",
    "\n",
    "# which kth neighbour is chosen for correct migration/incorrect migration\n",
    "# which kth neighbours are chosen for correct division vs. incorrect division\n",
    "kth_neighbour_dict = {\n",
    "    'ds_name': [],\n",
    "    'source_v': [],\n",
    "    'dest_v': [],\n",
    "    'edge_correct': [],\n",
    "    'neighbour_rank': []\n",
    "}\n",
    "for _, row in ds_summary.iterrows():\n",
    "    ds_name = row['ds_name']\n",
    "    solution_path = os.path.join(SOL_ROOT, ds_name, 'matched_solution.graphml')\n",
    "    all_edges_path = os.path.join(SOL_ROOT, ds_name, 'all_edges.csv')\n",
    "    \n",
    "    merge_gt = nx.read_graphml(solution_path, node_type=int)\n",
    "    all_edges = pd.read_csv(all_edges_path)\n",
    "    location_cols = ['z', 'y', 'x'] if 'z' in merge_gt.nodes[0] else ['y', 'x']\n",
    "    scale = scales[ds_name]['pixel_scale']\n",
    "\n",
    "    for edge in merge_gt.edges:\n",
    "        source = edge[0]\n",
    "        dest = edge[1]\n",
    "        edge_correct = not (merge_gt.edges[edge][EDGE_FP] or merge_gt.edges[edge][EDGE_WS])\n",
    "\n",
    "        other_edges = all_edges[(all_edges['u'] == source) & (all_edges['v'] >= 0)].sort_values(by='distance').reset_index()\n",
    "        neighbour_rank = other_edges[other_edges['v'] == dest].index[0]\n",
    "        kth_neighbour_dict['ds_name'].append(ds_name)\n",
    "        kth_neighbour_dict['source_v'].append(source)\n",
    "        kth_neighbour_dict['dest_v'].append(dest)\n",
    "        kth_neighbour_dict['edge_correct'].append(edge_correct)\n",
    "        kth_neighbour_dict['neighbour_rank'].append(neighbour_rank)\n",
    "kth_neighbour_df = pd.DataFrame(kth_neighbour_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "grid = sns.FacetGrid(kth_neighbour_df, col='ds_name', col_wrap=4, sharex=False, sharey=False, legend_out=True, hue='edge_correct')\n",
    "grid.map_dataframe(sns.countplot, x='neighbour_rank')\n",
    "grid.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "no_zeros = kth_neighbour_df[kth_neighbour_df['neighbour_rank'] > 0]\n",
    "grid = sns.FacetGrid(no_zeros, col='ds_name', col_wrap=4, sharex=False, sharey=False, legend_out=True)\n",
    "grid.map_dataframe(sns.countplot, x='neighbour_rank', hue='edge_correct')\n",
    "grid.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gt neighbour rank\n",
    "import os\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "GT_ROOT =  '/home/ddon0001/PhD/experiments/scaled/gt_no_div_constraint'\n",
    "kth_neighbour_dict = {\n",
    "    'ds_name': [],\n",
    "    'source_v': [],\n",
    "    'dest_v': [],\n",
    "    'neighbour_rank': []\n",
    "}\n",
    "for _, row in ds_summary.iterrows():\n",
    "    ds_name = row['ds_name']\n",
    "    solution_path = os.path.join(GT_ROOT, ds_name, 'matched_gt.graphml')\n",
    "    matching_path = os.path.join(GT_ROOT, ds_name, 'matching.json')\n",
    "    all_edges_path = os.path.join(GT_ROOT, ds_name, 'all_edges.csv')\n",
    "    \n",
    "    merge_gt = nx.read_graphml(solution_path)\n",
    "    all_edges = pd.read_csv(all_edges_path)\n",
    "    location_cols = ['x', 'y', 'z'] if 'z' in merge_gt.nodes[list(merge_gt.nodes)[0]] else ['x', 'y']\n",
    "\n",
    "    with open(matching_path) as f:\n",
    "        node_match = json.load(f)\n",
    "    gt_to_sol = {item[0]: item[1] for item in node_match}\n",
    "    sol_to_gt = {item[1]: item[0] for item in node_match}\n",
    "\n",
    "    for edge in merge_gt.edges:\n",
    "        source = edge[0]\n",
    "        dest = edge[1]\n",
    "\n",
    "        sol_source = gt_to_sol[source]\n",
    "        sol_dest = gt_to_sol[dest]\n",
    "        other_edges = all_edges[(all_edges['u'] == sol_source) & (all_edges['v'] >= 0)].sort_values(by='distance').reset_index()\n",
    "        v_edge = other_edges[other_edges['v'] == sol_dest]\n",
    "        if v_edge.empty:\n",
    "            neighbour_rank = -1\n",
    "        else:\n",
    "            neighbour_rank = v_edge.index[0]\n",
    "        kth_neighbour_dict['ds_name'].append(ds_name)\n",
    "        kth_neighbour_dict['source_v'].append(sol_source)\n",
    "        kth_neighbour_dict['dest_v'].append(sol_dest)\n",
    "        kth_neighbour_dict['neighbour_rank'].append(neighbour_rank)\n",
    "gt_neighbour_rank_df = pd.DataFrame(kth_neighbour_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "grid = sns.FacetGrid(gt_neighbour_rank_df[gt_neighbour_rank_df['neighbour_rank'] >= 0], col='ds_name', col_wrap=4, sharex=False, sharey=False, legend_out=True)\n",
    "grid.map_dataframe(sns.countplot, x='neighbour_rank')\n",
    "grid.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "grid = sns.FacetGrid(gt_neighbour_rank_df[gt_neighbour_rank_df['neighbour_rank'] > 0], col='ds_name', col_wrap=4, sharex=False, sharey=False, legend_out=True)\n",
    "grid.map_dataframe(sns.countplot, x='neighbour_rank')\n",
    "grid.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# investigating division costs as prop. of migration cost\n",
    "\n",
    "# cost of correct vs. incorrect edges for divisions vs. migration\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import os\n",
    "from tracktour._tracker import VirtualVertices\n",
    "\n",
    "chosen_cost_dict = {\n",
    "    'ds_name': [],\n",
    "    'div_parent_index': [],\n",
    "    'is_superparent': [],\n",
    "    'modified_cost': [],\n",
    "    'division_cost': [],\n",
    "    'chosen_children_dist': [],\n",
    "    # 'cheating_false', 'cheating_true', 'valid_false', 'valid_true'\n",
    "    'category': [],\n",
    "}\n",
    "for _, row in ds_summary.iterrows():\n",
    "    ds_name = row['ds_name']\n",
    "    solution_path = os.path.join(SOL_ROOT, ds_name, 'matched_solution.graphml')\n",
    "    all_edges_path = os.path.join(SOL_ROOT, ds_name, 'all_edges.csv')\n",
    "\n",
    "    merge_g = nx.read_graphml(solution_path, node_type=int)\n",
    "    all_edges_path = os.path.join(SOL_ROOT, ds_name, 'all_edges.csv')\n",
    "    location_cols = ['z', 'y', 'x'] if 'z' in merge_g.nodes[0] else ['y', 'x']\n",
    "    scale = scales[ds_name]['pixel_scale']\n",
    "\n",
    "    divs = [node for node in merge_g.nodes if merge_g.out_degree(node) > 1]\n",
    "    for div in divs:\n",
    "        children = list(merge_g.successors(div))\n",
    "        is_superparent = len(children) > 2\n",
    "        is_correct = not any(merge_g.edges[(div, child)][EDGE_FP] or merge_g.edges[(div, child)][EDGE_WS] for child in children)\n",
    "        parent_coords = np.asarray([merge_g.nodes[div][col] * scale[i] for i, col in enumerate(location_cols)])\n",
    "        child_coords = [np.asarray([merge_g.nodes[child][col] * scale[i] for i, col in enumerate(location_cols)]) for child in children]\n",
    "        distances = np.sort([np.linalg.norm(parent_coords - child) for child in child_coords])\n",
    "\n",
    "        closest_children_dist = distances[0] + distances[1]\n",
    "        div_edge = all_edges[(all_edges['u'] == VirtualVertices.DIV.value) & (all_edges['v'] == div)]\n",
    "        div_cost = div_edge['cost'].values[0]\n",
    "        is_valid = div_edge['flow'].values[0] > 0\n",
    "\n",
    "        if is_valid:\n",
    "            if is_correct:\n",
    "                category = 'valid_true'\n",
    "            else:\n",
    "                category = 'valid_false'\n",
    "        else:\n",
    "            if is_correct:\n",
    "                category = 'cheating_true'\n",
    "            else:\n",
    "                category = 'cheating_false'\n",
    "\n",
    "        chosen_cost_dict['ds_name'].append(ds_name)\n",
    "        chosen_cost_dict['div_parent_index'].append(div)\n",
    "        chosen_cost_dict['is_superparent'].append(is_superparent)\n",
    "        # what division cost was incorrectly not chosen? i.e. the false div cost of merged divs?\n",
    "        chosen_cost_dict['modified_cost'].append(div_cost if is_valid else closest_children_dist)\n",
    "        chosen_cost_dict['division_cost'].append(div_cost)\n",
    "        chosen_cost_dict['chosen_children_dist'].append(closest_children_dist)\n",
    "        chosen_cost_dict['category'].append(category)\n",
    "\n",
    "chosen_cost_df = pd.DataFrame(chosen_cost_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "grid = sns.FacetGrid(chosen_cost_df, col='ds_name', col_wrap=4, sharex=False, sharey=False, legend_out=True, hue='category')\n",
    "grid.map_dataframe(sns.scatterplot, x='modified_cost', y='chosen_children_dist')\n",
    "grid.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "grid = sns.FacetGrid(chosen_cost_df, col='ds_name', col_wrap=4, sharex=False, sharey=False, legend_out=True, hue='category')\n",
    "grid.map_dataframe(sns.scatterplot, x='division_cost', y='chosen_children_dist', alpha=0.4)\n",
    "grid.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "chosen_cost_df['is_correct'] = chosen_cost_df.category.str.contains('true')\n",
    "chosen_cost_df['is_valid'] = chosen_cost_df.category.str.contains('valid')\n",
    "grid = sns.FacetGrid(chosen_cost_df, col='ds_name', col_wrap=4, sharex=False, sharey=False, legend_out=True, hue='is_correct')\n",
    "grid.map_dataframe(sns.scatterplot, x='division_cost', y='chosen_children_dist', alpha=0.4)\n",
    "grid.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = sns.FacetGrid(chosen_cost_df, col='ds_name', col_wrap=4, sharex=False, sharey=False, legend_out=True, hue='is_valid')\n",
    "grid.map_dataframe(sns.scatterplot, x='division_cost', y='chosen_children_dist', alpha=0.3)\n",
    "grid.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solution_path = os.path.join(SOL_ROOT, 'Fluo-N3DH-CE_02', 'matched_solution.graphml')\n",
    "all_edges_path = os.path.join(SOL_ROOT, 'Fluo-N3DH-CE_02', 'all_edges.csv')\n",
    "\n",
    "merge_g = nx.read_graphml(solution_path, node_type=int)\n",
    "all_edges = pd.read_csv(all_edges_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# structural property of tracks - mergeless tracks\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import networkx as nx\n",
    "\n",
    "all_components = dict()\n",
    "for _, row in ds_summary.iterrows():\n",
    "    ds_name = row['ds_name']\n",
    "    solution_path = os.path.join(SOL_ROOT, ds_name, 'matched_solution.graphml')\n",
    "    all_edges_path = os.path.join(SOL_ROOT, ds_name, 'all_edges.csv')\n",
    "\n",
    "    merge_g = nx.read_graphml(solution_path, node_type=int)\n",
    "    all_edges = pd.read_csv(all_edges_path)\n",
    "\n",
    "    undirected_g = merge_g.to_undirected()\n",
    "    components = defaultdict(dict)\n",
    "    for i, component in enumerate(nx.connected_components(undirected_g)):\n",
    "        components[i]['nodes'] = component\n",
    "        components[i]['edges'] = list(merge_g.subgraph(component).edges)\n",
    "        components[i]['total_edges'] = len(components[i]['edges'])\n",
    "        components[i]['count_nodes'] = len(component)\n",
    "\n",
    "        has_merges = [merge_g.in_degree(node) > 1 for node in component]\n",
    "        count_merges = sum(has_merges)\n",
    "        components[i]['has_merge'] = count_merges > 0\n",
    "        components[i]['count_merges'] = count_merges\n",
    "\n",
    "        has_divisions = [merge_g.out_degree(node) > 1 for node in component]\n",
    "        count_divisions = sum(has_divisions)\n",
    "        components[i]['has_division'] = count_divisions > 0\n",
    "        components[i]['count_divisions'] = count_divisions\n",
    "\n",
    "        wrong_edge = [(merge_g.edges[edge][EDGE_FP] or merge_g.edges[edge][EDGE_WS]) for edge in components[i]['edges']]\n",
    "        components[i]['count_wrong_edges'] = wrong_edge.count(True)\n",
    "        components[i]['count_correct_edges'] = wrong_edge.count(False)\n",
    "        components[i]['is_correct'] = components[i]['count_correct_edges'] == components[i]['total_edges']\n",
    "    all_components[ds_name] = components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_name, component_id, has_merge, is_fully_correct, prop_correct_edges\n",
    "ds_names = []\n",
    "comp_ids = []\n",
    "has_merge = []\n",
    "count_merge = []\n",
    "has_div = []\n",
    "count_div = []\n",
    "is_fully_correct = []\n",
    "prop_correct_edges = []\n",
    "total_edges = []\n",
    "for ds_name, comp_info in all_components.items():\n",
    "    for comp_id, comp in comp_info.items():\n",
    "        ds_names.append(ds_name)\n",
    "        comp_ids.append(comp_id)\n",
    "        has_merge.append(comp['has_merge'])\n",
    "        count_merge.append(comp['count_merges'])\n",
    "        has_div.append(comp['has_division'])\n",
    "        count_div.append(comp['count_divisions'])\n",
    "        is_fully_correct.append(comp['is_correct'])\n",
    "        if comp['total_edges']> 0:\n",
    "            prop_correct_edges.append(comp['count_correct_edges']/comp['total_edges'])\n",
    "        else:\n",
    "            prop_correct_edges.append(0)\n",
    "        total_edges.append(comp['total_edges'])\n",
    "component_df = pd.DataFrame({\n",
    "    'ds_name': ds_names,\n",
    "    'component': comp_ids,\n",
    "    'has_merge': has_merge,\n",
    "    'count_merges': count_merge,\n",
    "    'has_div': has_div,\n",
    "    'count_div': count_div,\n",
    "    'is_correct': is_fully_correct,\n",
    "    'prop_correct': prop_correct_edges,\n",
    "    'total_edges': total_edges\n",
    "})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from tracktour._tracker import VirtualVertices\n",
    "\n",
    "ds_names = []\n",
    "node_ids = []\n",
    "node_areas = []\n",
    "comp_ids = []\n",
    "# true if component contains a merge\n",
    "merge_comp = []\n",
    "# true if component contains a div\n",
    "div_comp = []\n",
    "# true if component is entirely correct\n",
    "is_correct_comp = []\n",
    "# true if node is a parent of a division\n",
    "is_parent = []\n",
    "# true if node is a parent of 3+ children\n",
    "is_superparent = []\n",
    "# true if the node has two incoming edges\n",
    "is_merge_vertex = []\n",
    "# -1 if node is not a parent, otherwise difference in distance to (two closest) children\n",
    "child_distance_difference = []\n",
    "# -1 if node is not a parent, otherwise sum of distances to (two closest) children\n",
    "child_distance_sum = []\n",
    "# -1 if node is not a parent, otherwise distance between two closest children\n",
    "interchild_distance = []\n",
    "# -1 if node is not a parent, otherwise angle between two closest children\n",
    "# arcos of dot product of vectors from parent to children scaled by product of magnitudes\n",
    "div_angle = []\n",
    "# True if node is parent and both children are correct, otherwise False\n",
    "div_correct = []\n",
    "# True if node is parent and there is division flow into node\n",
    "div_valid = []\n",
    "# -1 if node has no successors, otherwise distance rank of the first child\n",
    "first_chosen_neighbour_rank = []\n",
    "# -1 if node is not a parent, otherwise distance rank of the second child\n",
    "second_chosen_neighbour_rank = []\n",
    "# -1 if node has no successors, otherwise distance to the closest child\n",
    "first_child_distance = []\n",
    "# -1 if node is not a parent, otherwise distance to the second closest child\n",
    "second_child_distance = []\n",
    "# -1 if node has no successors, otherwise area of the first child\n",
    "first_child_area = []\n",
    "# -1 if node is not a parent, otherwise area of the second child\n",
    "second_child_area = []\n",
    "# sum of areas of two children if node is parent, otherwise -1\n",
    "child_area_sum = []\n",
    "# cost of the Dv edge for node v (regardless of whether it was used)\n",
    "div_cost = []\n",
    "# True if node is not a parent, and the edge to its child is correct, otherwise False\n",
    "mig_correct = []\n",
    "for _, row in ds_summary.iterrows():\n",
    "    ds_name = row['ds_name']\n",
    "    det_path = row['det_path']\n",
    "    solution_path = os.path.join(SOL_ROOT, ds_name, 'matched_solution.graphml')\n",
    "    all_edges_path = os.path.join(SOL_ROOT, ds_name, 'all_edges.csv')\n",
    "\n",
    "    merge_g = nx.read_graphml(solution_path, node_type=int)\n",
    "    all_edges = pd.read_csv(all_edges_path)\n",
    "    det_df = pd.read_csv(det_path)\n",
    "    location_cols = ['z', 'y', 'x'] if 'z' in merge_g.nodes[0] else ['y', 'x']\n",
    "    scale = scales[ds_name]['pixel_scale']\n",
    "\n",
    "    ds_components = all_components[ds_name]\n",
    "    for comp_id, comp in ds_components.items():\n",
    "        for node in comp['nodes']:\n",
    "            ds_names.append(ds_name)\n",
    "            node_ids.append(node)\n",
    "            node_areas.append(det_df.loc[node].area)\n",
    "            comp_ids.append(comp_id)\n",
    "            merge_comp.append(comp['has_merge'])\n",
    "            div_comp.append(comp['has_division'])\n",
    "            is_correct_comp.append(comp['is_correct'])\n",
    "            is_parent.append(merge_g.out_degree(node) > 1)\n",
    "            is_superparent.append(merge_g.out_degree(node) > 2)\n",
    "            is_merge_vertex.append(merge_g.in_degree(node) > 1)\n",
    "\n",
    "            children = list(merge_g.successors(node))\n",
    "            parent_coords = np.asarray([merge_g.nodes[node][col] * scale[i] for i, col in enumerate(location_cols)])\n",
    "            child_coords = [np.asarray([merge_g.nodes[child][col] * scale[i] for i, col in enumerate(location_cols)]) for child in children]\n",
    "            child_distances = [np.linalg.norm(parent_coords - child) for child in child_coords]\n",
    "            dist_indices = np.argsort(child_distances)\n",
    "            children = [children[i] for i in dist_indices]\n",
    "            closest_children = [child_coords[i] for i in dist_indices]\n",
    "            distances = [child_distances[i] for i in dist_indices]\n",
    "\n",
    "            child_dist_diff = -1\n",
    "            child_dist_sum = -1\n",
    "            ichild_dist = -1\n",
    "            ang = -1\n",
    "            first_neighb = -1\n",
    "            second_neighb = -1\n",
    "            first_dist = -1\n",
    "            second_dist = -1\n",
    "            first_area = -1\n",
    "            second_area = -1\n",
    "            area_sum = -1\n",
    "            div_cos = -1\n",
    "            div_corr = False\n",
    "            div_val = False\n",
    "            mig_corr = False\n",
    "            if len(children) > 0:\n",
    "                # save first child stuff\n",
    "                other_edges = all_edges[(all_edges['u'] == node) & (all_edges['v'] >= 0)].sort_values(by='distance').reset_index()\n",
    "                neighbour_ranks = [other_edges[other_edges['v'] == dest].index[0] for dest in children]\n",
    "                first_neighb = neighbour_ranks[0]\n",
    "                first_dist = distances[0]\n",
    "                first_area = det_df.loc[children[0]].area\n",
    "                if len(children) == 1:\n",
    "                    mig_corr = (not (merge_g.edges[(node, children[0])][EDGE_FP] or merge_g.edges[(node, children[0])][EDGE_WS]))\n",
    "                else:\n",
    "                    ang = np.degrees(np.arccos(\n",
    "                        np.dot(closest_children[0] - parent_coords, closest_children[1] - parent_coords) /\n",
    "                        (distances[0] * distances[1])\n",
    "                    ))\n",
    "                    div_corr = not any(merge_g.edges[(node, child)][EDGE_FP] or merge_g.edges[(node, child)][EDGE_WS] for child in children)\n",
    "                    second_neighb = neighbour_ranks[1]\n",
    "                    second_dist = distances[1]\n",
    "                    second_area = det_df.loc[children[1]].area\n",
    "                    area_sum = first_area + second_area\n",
    "                    child_dist_diff = distances[1] - distances[0]\n",
    "                    child_dist_sum = distances[1] + distances[0]\n",
    "                    ichild_dist = np.linalg.norm(closest_children[0] - closest_children[1])\n",
    "                    div_edge = all_edges[(all_edges['u'] == VirtualVertices.DIV.value) & (all_edges['v'] == node)]\n",
    "                    if len(div_edge):\n",
    "                        div_cos = div_edge['cost'].values[0]\n",
    "                        div_val = div_edge['flow'].values[0] > 0\n",
    "            child_distance_difference.append(child_dist_diff)\n",
    "            child_distance_sum.append(child_dist_sum)\n",
    "            interchild_distance.append(ichild_dist)\n",
    "            div_angle.append(ang)\n",
    "            child_area_sum.append(area_sum)\n",
    "            first_chosen_neighbour_rank.append(first_neighb)\n",
    "            second_chosen_neighbour_rank.append(second_neighb)\n",
    "            first_child_distance.append(first_dist)\n",
    "            first_child_area.append(first_area)\n",
    "            second_child_distance.append(second_dist)\n",
    "            second_child_area.append(second_area)\n",
    "            div_cost.append(div_cos)\n",
    "            div_correct.append(div_corr)\n",
    "            div_valid.append(div_val)\n",
    "            mig_correct.append(mig_corr)\n",
    "all_node_info = pd.DataFrame({\n",
    "    'ds_name': ds_names,\n",
    "    'node_id': node_ids,\n",
    "    'node_area': node_areas,\n",
    "    'component_id': comp_ids,\n",
    "    'comp_merge': merge_comp,\n",
    "    'comp_div': div_comp,\n",
    "    'comp_correct': is_correct_comp,\n",
    "    'is_parent': is_parent,\n",
    "    'is_superparent': is_superparent,\n",
    "    'is_merge_vertex': is_merge_vertex,\n",
    "    'child_distance_difference': child_distance_difference,\n",
    "    'child_distance_sum': child_distance_sum,\n",
    "    'interchild_distance': interchild_distance,\n",
    "    'div_angle': div_angle,\n",
    "    'child_area_sum': child_area_sum,\n",
    "    'div_correct': div_correct,\n",
    "    'div_valid': div_valid,\n",
    "    'first_chosen_neighbour_rank': first_chosen_neighbour_rank,\n",
    "    'second_chosen_neighbour_rank': second_chosen_neighbour_rank,\n",
    "    'first_child_distance': first_child_distance,\n",
    "    'first_child_area': first_child_area,\n",
    "    'second_child_distance': second_child_distance,\n",
    "    'second_child_area': second_child_area,\n",
    "    'div_cost': div_cost,\n",
    "    'mig_correct': mig_correct\n",
    "})\n",
    "# all_node_info.to_csv(os.path.join(SOL_ROOT, 'all_node_info.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "grid = sns.FacetGrid(component_df, col='ds_name', col_wrap=4, sharex=True, sharey=False, legend_out=True, hue='has_merge')\n",
    "grid.map_dataframe(sns.countplot, x='is_correct')\n",
    "grid.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "grid = sns.FacetGrid(component_df[component_df.has_merge == False], col='ds_name', col_wrap=4, sharex=True, sharey=False, legend_out=True, hue='has_div')\n",
    "grid.map_dataframe(sns.countplot, x='is_correct')\n",
    "grid.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = sns.FacetGrid(component_df[(component_df.has_div == False) & (component_df.has_merge == False)], col='ds_name', col_wrap=4, sharex=True, sharey=False, legend_out=True)\n",
    "grid.map_dataframe(sns.countplot, x='is_correct')\n",
    "grid.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = sns.FacetGrid(component_df[(component_df.has_div == True) & (component_df.has_merge == False)], col='ds_name', col_wrap=4, sharex=True, sharey=False, legend_out=True)\n",
    "grid.map_dataframe(sns.countplot, x='is_correct')\n",
    "grid.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "grid = sns.FacetGrid(component_df, col='ds_name', col_wrap=4, sharex=False, sharey=False, legend_out=True, hue='has_merge')\n",
    "grid.map_dataframe(sns.histplot, x='total_edges')\n",
    "grid.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "grid = sns.FacetGrid(component_df[component_df['total_edges'] > 0], col='ds_name', col_wrap=4, sharex=False, sharey=False, legend_out=True, hue='has_merge')\n",
    "grid.map_dataframe(sns.scatterplot, x='total_edges', y='prop_correct')\n",
    "grid.add_legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What else do we want to know about merge vs. mergeless components\n",
    "\n",
    "- num divisions (correct/incorrect)\n",
    "- division asymmetry coloured by correct/incorrect merge/mergeless\n",
    "- angle of division coloured by correct/incorrect merge/mergeless\n",
    "- sum of cell area vs. parent area coloured by correct/incorrect\n",
    "- would be good to learn a migration measure from tracks with no divisions\n",
    "    - how many of these've we got?\n",
    "- distance from borders?\n",
    "- specifically looking for discriminators within the mergeless components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_info_df = pd.read_csv(os.path.join(SOL_ROOT, 'all_node_info.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_info_df['child_area_prop'] = np.where(all_info_df['child_area_sum'] > 0, all_info_df['child_area_sum'] / all_info_df['node_area'], -1)\n",
    "all_info_df['neighbour_rank_difference'] = np.where(all_info_df['second_chosen_neighbour_rank'] >= 0, all_info_df['second_chosen_neighbour_rank'] - all_info_df['first_chosen_neighbour_rank'], -1)\n",
    "all_info_df['neighbour_rank_sum'] = np.where(all_info_df['second_chosen_neighbour_rank'] >= 0, all_info_df['second_chosen_neighbour_rank'] + all_info_df['first_chosen_neighbour_rank'], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_info_df.to_csv(os.path.join(SOL_ROOT, 'all_node_info.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "grid = sns.FacetGrid(all_info_df[all_info_df.is_parent], col='ds_name', col_wrap=4, sharex=False, sharey=False, legend_out=True, hue='div_correct')\n",
    "grid.map_dataframe(sns.histplot, x='neighbour_rank_sum')\n",
    "grid.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_info_df[all_info_df.first_child_distance >= 0].value_counts('mig_correct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_info_df[all_info_df.second_child_distance >= 0].value_counts('div_correct')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Appearance/Disappearance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import json\n",
    "import os\n",
    "\n",
    "sol_root = '/home/ddon0001/PhD/experiments/scaled/no_div_constraint_err_seg'\n",
    "ds_summary = pd.read_csv(os.path.join(sol_root, 'summary.csv'))\n",
    "ds_names = ds_summary.ds_name.unique() \n",
    "\n",
    "ad_ds_names = []\n",
    "app_counts = []\n",
    "sol_app_counts = []\n",
    "disapp_counts = []\n",
    "sol_disapp_counts = []\n",
    "fn_app_counts = []\n",
    "fn_disapp_counts = []\n",
    "for ds_name in ds_names:\n",
    "    solution_path = os.path.join(sol_root, ds_name, 'matched_solution.graphml')\n",
    "    gt_solution_path = os.path.join(sol_root, ds_name, 'matched_gt.graphml')\n",
    "    matching_path = os.path.join(sol_root, ds_name, 'matching.json')\n",
    "\n",
    "    merge_g = nx.read_graphml(solution_path, node_type=int)\n",
    "    gt_g = nx.read_graphml(gt_solution_path)\n",
    "    with open(matching_path) as f:\n",
    "        node_match = json.load(f)\n",
    "        gt_to_sol = {item[0]: item[1] for item in node_match}\n",
    "        sol_to_gt = {item[1]: item[0] for item in node_match}\n",
    "    \n",
    "    start_frame = 0\n",
    "    end_frame = ds_summary[ds_summary.ds_name == ds_name].n_frames.values[0] - 1\n",
    "\n",
    "    appearances = [node for node in gt_g.nodes if gt_g.in_degree(node) == 0 and gt_g.nodes[node]['t'] != 0]\n",
    "    disappearances = [node for node in gt_g.nodes if gt_g.out_degree(node) == 0 and gt_g.nodes[node]['t'] != end_frame]\n",
    "    \n",
    "    is_fn_app = [node for node in appearances if node not in gt_to_sol]\n",
    "    is_fn_disapp = [node for node in disappearances if node not in gt_to_sol]\n",
    "\n",
    "    sol_apps = [node for node in merge_g.nodes if merge_g.in_degree(node) == 0 and merge_g.nodes[node]['t'] != 0]\n",
    "    sol_disapps = [node for node in merge_g.nodes if merge_g.out_degree(node) == 0 and merge_g.nodes[node]['t'] != end_frame]\n",
    "    \n",
    "    appearance_count = len(appearances)\n",
    "    disappearance_count = len(disappearances)\n",
    "\n",
    "    fn_app_count = len(is_fn_app)\n",
    "    fn_disapp_count = len(is_fn_disapp)\n",
    "\n",
    "    sol_app_count = len(sol_apps)\n",
    "    sol_disapp_count = len(sol_disapps)\n",
    "\n",
    "    ad_ds_names.append(ds_name)\n",
    "\n",
    "    app_counts.append(appearance_count)\n",
    "    disapp_counts.append(disappearance_count)\n",
    "\n",
    "    fn_app_counts.append(fn_app_count)\n",
    "    fn_disapp_counts.append(fn_disapp_count)\n",
    "\n",
    "    sol_app_counts.append(sol_app_count)\n",
    "    sol_disapp_counts.append(sol_disapp_count)\n",
    "\n",
    "app_disapp_df = pd.DataFrame({\n",
    "    'ds_name': ad_ds_names,\n",
    "    'appearance_count': app_counts,\n",
    "    'sol_app_count': sol_app_counts,\n",
    "    'fn_app_count': fn_app_counts,\n",
    "    'disappearance_count': disapp_counts,\n",
    "    'sol_disapp_count': sol_disapp_counts,\n",
    "    'fn_disapp_count': fn_disapp_counts\n",
    "})\n",
    "app_disapp_df.sort_values(by='disappearance_count', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn\n",
    "\n",
    "def get_appearance_prop(row):\n",
    "    if row['appearance_count'] == 0:\n",
    "        if row['sol_app_count'] == 0:\n",
    "            return 1\n",
    "        else:\n",
    "            return row['sol_app_count']\n",
    "    return row['sol_app_count'] / row['appearance_count']\n",
    "\n",
    "def get_disappearance_prop(row):\n",
    "    if row['disappearance_count'] == 0:\n",
    "        if row['sol_disapp_count'] == 0:\n",
    "            return 1\n",
    "        else:\n",
    "            return row['sol_disapp_count']\n",
    "    return row['sol_disapp_count'] / row['disappearance_count']\n",
    "\n",
    "app_disapp_df['appearance_diff_prop'] = app_disapp_df.apply(get_appearance_prop, axis=1)\n",
    "app_disapp_df['disappearance_diff_prop'] = app_disapp_df.apply(get_disappearance_prop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_disapp_df[['ds_name', 'appearance_diff_prop']].sort_values(by='appearance_diff_prop', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_disapp_df[['ds_name', 'disappearance_diff_prop']].sort_values(by='disappearance_diff_prop', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.histplot(app_disapp_df, x='appearance_diff_prop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(app_disapp_df, x='disappearance_diff_prop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from tracktour._io_util import read_scale\n",
    "\n",
    "def get_gt_coords(gt_node_info):\n",
    "    if 'z' in gt_node_info:\n",
    "        return np.asarray([gt_node_info['x'], gt_node_info['y'], gt_node_info['z']])\n",
    "    else:\n",
    "        return np.asarray([gt_node_info['x'], gt_node_info['y']])\n",
    "\n",
    "sol_root = '/home/ddon0001/PhD/experiments/scaled/no_div_constraint_err_seg'\n",
    "ds_summary = pd.read_csv(os.path.join(sol_root, 'summary.csv'))\n",
    "ds_names = ds_summary.ds_name.unique()\n",
    "\n",
    "ap_ds_names = []\n",
    "ap_gt_id = []\n",
    "ap_sol_id = []\n",
    "ap_cost = []\n",
    "ap_is_fn = []\n",
    "ap_is_correct = []\n",
    "ap_real_cost = []\n",
    "\n",
    "disap_ds_names = []\n",
    "disap_gt_id = []\n",
    "disap_sol_id = []\n",
    "disap_cost = []\n",
    "disap_is_fn = []\n",
    "disap_is_correct = []\n",
    "disap_real_cost = []\n",
    "for ds_name in ds_names:\n",
    "    solution_path = os.path.join(sol_root, ds_name, 'matched_solution.graphml')\n",
    "    gt_solution_path = os.path.join(sol_root, ds_name, 'matched_gt.graphml')\n",
    "    matching_path = os.path.join(sol_root, ds_name, 'matching.json')\n",
    "    all_edges_path = os.path.join(sol_root, ds_name, 'all_edges.csv')\n",
    "    scale = read_scale(ds_name)\n",
    "\n",
    "    merge_g = nx.read_graphml(solution_path, node_type=int)\n",
    "    gt_g = nx.read_graphml(gt_solution_path)\n",
    "    with open(matching_path) as f:\n",
    "        node_match = json.load(f)\n",
    "        gt_to_sol = {item[0]: item[1] for item in node_match}\n",
    "        sol_to_gt = {item[1]: item[0] for item in node_match}\n",
    "    all_edges = pd.read_csv(all_edges_path)\n",
    "    \n",
    "    start_frame = 0\n",
    "    ds_row = ds_summary[ds_summary.ds_name == ds_name]\n",
    "    end_frame = ds_row.n_frames.values[0] - 1\n",
    "    im_shape = np.asarray(eval(ds_row.im_shape.values[0]))\n",
    "    for i, elem in enumerate(im_shape):\n",
    "        im_shape[i] = elem * scale[i]\n",
    "\n",
    "    appearances = [node for node in gt_g.nodes if gt_g.in_degree(node) == 0 and gt_g.nodes[node]['t'] != 0]\n",
    "    disappearances = [node for node in gt_g.nodes if gt_g.out_degree(node) == 0 and gt_g.nodes[node]['t'] != end_frame]\n",
    "\n",
    "    for app in appearances:\n",
    "        ap_gt_id.append(app)\n",
    "        ap_ds_names.append(ds_name)\n",
    "        if app in gt_to_sol:\n",
    "            ap_sol_id.append(gt_to_sol[app])\n",
    "            ap_is_fn.append(False)\n",
    "            cost = all_edges[(all_edges['u'] == -2) & (all_edges['v'] == gt_to_sol[app])]['cost'].values[0]\n",
    "            if merge_g.in_degree(gt_to_sol[app]) == 0:\n",
    "                ap_is_correct.append(True)\n",
    "                ap_real_cost.append(cost)\n",
    "            else:\n",
    "                ap_is_correct.append(False)\n",
    "                ap_real_cost.append(min([merge_g.edges[edge]['cost'] for edge in merge_g.in_edges(gt_to_sol[app])]))\n",
    "        else:\n",
    "            ap_sol_id.append(-1)\n",
    "            ap_is_fn.append(True)\n",
    "            dist_to_top_left = get_gt_coords(gt_g.nodes[app])\n",
    "            for i, coords in enumerate(dist_to_top_left):\n",
    "                dist_to_top_left[i] = scale[i] * coords\n",
    "            dist_to_bottom_right = im_shape - dist_to_top_left\n",
    "            cost = np.min(np.concatenate([dist_to_top_left, dist_to_bottom_right]))\n",
    "            ap_is_correct.append(False)\n",
    "            ap_real_cost.append(-1)\n",
    "        ap_cost.append(cost)\n",
    "\n",
    "    for disap in disappearances:\n",
    "        disap_gt_id.append(disap)\n",
    "        disap_ds_names.append(ds_name)\n",
    "        if disap in gt_to_sol:\n",
    "            disap_sol_id.append(gt_to_sol[disap])\n",
    "            disap_is_fn.append(False)\n",
    "            cost = all_edges[(all_edges['u'] == gt_to_sol[disap]) & (all_edges['v'] == -4)]['cost'].values[0]\n",
    "            if merge_g.out_degree(gt_to_sol[disap]) == 0:\n",
    "                disap_is_correct.append(True)\n",
    "                disap_real_cost.append(cost)\n",
    "            else:\n",
    "                disap_is_correct.append(False)\n",
    "                disap_real_cost.append(min([merge_g.edges[edge]['cost'] for edge in merge_g.out_edges(gt_to_sol[disap])]))\n",
    "        else:\n",
    "            disap_sol_id.append(-1)\n",
    "            disap_is_fn.append(True)\n",
    "            dist_to_top_left = get_gt_coords(gt_g.nodes[disap])\n",
    "            for i, coords in enumerate(dist_to_top_left):\n",
    "                dist_to_top_left[i] = scale[i] * coords\n",
    "            dist_to_bottom_right = im_shape - dist_to_top_left\n",
    "            cost = np.min(np.concatenate([dist_to_top_left, dist_to_bottom_right]))\n",
    "            disap_is_correct.append(False)\n",
    "            disap_real_cost.append(-1)\n",
    "        disap_cost.append(cost)\n",
    "ap_df = pd.DataFrame({\n",
    "    'ds_name': ap_ds_names,\n",
    "    'gt_id': ap_gt_id,\n",
    "    'sol_id': ap_sol_id,\n",
    "    'cost': ap_cost,\n",
    "    'is_fn': ap_is_fn,\n",
    "    'is_correct': ap_is_correct,\n",
    "    # min cost of all incoming edges, -1 for fn appearing vertices\n",
    "    'real_cost': ap_real_cost\n",
    "})\n",
    "disap_df = pd.DataFrame({\n",
    "    'ds_name': disap_ds_names,\n",
    "    'gt_id': disap_gt_id,\n",
    "    'sol_id': disap_sol_id,\n",
    "    'cost': disap_cost,\n",
    "    'is_fn': disap_is_fn,\n",
    "    'is_correct': disap_is_correct,\n",
    "    # min cost of all outgoing edges, -1 for fn disappearing vertices\n",
    "    'real_cost': disap_real_cost\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap_df.to_csv('/home/ddon0001/PhD/experiments/appearance_disappearance/appearances.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disap_df.to_csv('/home/ddon0001/PhD/experiments/appearance_disappearance/disappearances.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "grid = sns.FacetGrid(ap_df, col='ds_name', col_wrap=4, sharex=False, sharey=False, legend_out=True, hue='is_correct')\n",
    "grid.map_dataframe(sns.histplot, x='cost')\n",
    "grid.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "grid = sns.FacetGrid(disap_df, col='ds_name', col_wrap=4, sharex=False, sharey=False, legend_out=True, hue='is_correct')\n",
    "grid.map_dataframe(sns.histplot, x='cost')\n",
    "grid.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_true_costs_ap = ap_df[(ap_df.is_fn == False) & (ap_df.is_correct == False)]\n",
    "import seaborn as sns\n",
    "\n",
    "only_true_costs_ap['cost_diff'] = only_true_costs_ap['cost'] - only_true_costs_ap['real_cost']\n",
    "\n",
    "grid = sns.FacetGrid(only_true_costs_ap, col='ds_name', col_wrap=4, sharex=False, sharey=False, legend_out=True)\n",
    "grid.map_dataframe(sns.histplot, x='cost_diff')\n",
    "grid.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_true_costs_disap = disap_df[(disap_df.is_fn == False) & (disap_df.is_correct == False)]\n",
    "import seaborn as sns\n",
    "\n",
    "only_true_costs_disap['cost_diff'] = only_true_costs_disap['cost'] - only_true_costs_disap['real_cost']\n",
    "\n",
    "grid = sns.FacetGrid(only_true_costs_disap, col='ds_name', col_wrap=4, sharex=False, sharey=False, legend_out=True)\n",
    "grid.map_dataframe(sns.histplot, x='cost_diff')\n",
    "grid.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: investigate the length of a track segment before disappearance\n",
    "# TODO: look at some appearances - why aren't they divisions? are they more likely in 3D data than in 2D?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap_df['reason'] = 'unknown'\n",
    "ap_df['note'] = 'N/A'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reasons for Appearance\n",
    "\n",
    "Cells should mostly be appearing through division, but other possible reasons for appearance are:\n",
    "\n",
    "- Entering from sides\n",
    "- Disocclusion: we saw this cell, it hid behind a friend, it came back out\n",
    "- Emergence: it was poorly stained or too deep in a 3D dataset and now it has appeared sufficiently clearly to be segmented\n",
    "\n",
    "We'll manually categorize some of these below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cells might be disappearing due to:\n",
    "\n",
    "- Exiting the frame\n",
    "- Becoming occluded full by some other cell\n",
    "- Dying/losing stain/becoming undetectable from the background (we refer to all of these as death)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "ap_df = pd.read_csv('/home/ddon0001/PhD/experiments/appearance_disappearance/disappearances.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_names = ap_df.ds_name.unique()\n",
    "ds_name_of_interest = 'PhC-C2DL-PSC_02'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_of_interest = ap_df[ap_df.ds_name == ds_name_of_interest]\n",
    "len(ds_of_interest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading TIFFs: 100%|| 299/299 [00:00<00:00, 384.64it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<napari._qt.widgets.qt_viewer_dock_widget.QtViewerDockWidget at 0x7fb6cf7d2200>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import napari\n",
    "from tracktour import load_tiff_frames\n",
    "import networkx as nx\n",
    "from magicgui import magicgui\n",
    "from skimage.measure import regionprops_table\n",
    "\n",
    "ds, seq = ds_name_of_interest.split('_')\n",
    "\n",
    "sol_root = f'/home/ddon0001/PhD/experiments/scaled/no_div_constraint_err_seg/{ds_name_of_interest}/matched_solution.graphml'\n",
    "gt_root = f'/home/ddon0001/PhD/data/cell_tracking_challenge/SUBMISSION/{ds}/{seq}_GT/TRA/'\n",
    "im_root = f'/home/ddon0001/PhD/data/cell_tracking_challenge/SUBMISSION/{ds}/{seq}/'\n",
    "\n",
    "# load solution into napari, browse, assign reason\n",
    "im = load_tiff_frames(im_root)\n",
    "tra_seg = load_tiff_frames(gt_root)\n",
    "sol = nx.read_graphml(sol_root, node_type=int)\n",
    "\n",
    "viewer = napari.Viewer()\n",
    "viewer.add_image(im)\n",
    "viewer.add_labels(tra_seg)\n",
    "\n",
    "row_idx = ds_of_interest.index[0]\n",
    "\n",
    "@magicgui(\n",
    "    call_button='Next',\n",
    "    result_widget=True\n",
    ")\n",
    "def iter_through_nodes(labels: 'napari.layers.Labels', viewer: 'napari.Viewer'):\n",
    "    global row_idx\n",
    "    if row_idx <= ds_of_interest.index[-1]:\n",
    "        row = ds_of_interest.loc[row_idx]\n",
    "        gt_node_id = row['gt_id']\n",
    "        label, t = (int(item) for item in gt_node_id.split('_'))\n",
    "        labels.selected_label = label\n",
    "        viewer.dims.current_step = (t, ) + tuple([0 for _ in range(len(labels.data.shape) - 1)])\n",
    "        label_mask = (labels.data[t] == label).astype(int)\n",
    "        centroid_info = regionprops_table(label_mask, properties=['centroid'])\n",
    "        centroid = (t, )\n",
    "        for i in range(len(labels.data.shape) - 1):\n",
    "            centroid += (centroid_info[f'centroid-{i}'][0], )\n",
    "        viewer.camera.center = centroid\n",
    "        viewer.camera.zoom = 3\n",
    "    row_idx += 1\n",
    "    return f'{row_idx - 1} --- label: {label}, t: {t},'\n",
    "\n",
    "viewer.window.add_dock_widget(iter_through_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds_name</th>\n",
       "      <th>gt_id</th>\n",
       "      <th>sol_id</th>\n",
       "      <th>cost</th>\n",
       "      <th>is_fn</th>\n",
       "      <th>is_correct</th>\n",
       "      <th>real_cost</th>\n",
       "      <th>reason</th>\n",
       "      <th>note</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>PhC-C2DL-PSC_02</td>\n",
       "      <td>695_209</td>\n",
       "      <td>24945</td>\n",
       "      <td>35.093333</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>33.022967</td>\n",
       "      <td>exiting</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>PhC-C2DL-PSC_02</td>\n",
       "      <td>735_144</td>\n",
       "      <td>12848</td>\n",
       "      <td>33.972727</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>35.035729</td>\n",
       "      <td>exiting</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>PhC-C2DL-PSC_02</td>\n",
       "      <td>744_161</td>\n",
       "      <td>15345</td>\n",
       "      <td>33.141573</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>22.597066</td>\n",
       "      <td>exiting</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>PhC-C2DL-PSC_02</td>\n",
       "      <td>749_93</td>\n",
       "      <td>7037</td>\n",
       "      <td>30.245161</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>30.245161</td>\n",
       "      <td>exiting</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>PhC-C2DL-PSC_02</td>\n",
       "      <td>764_19</td>\n",
       "      <td>1249</td>\n",
       "      <td>27.730000</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>27.730000</td>\n",
       "      <td>exiting</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>PhC-C2DL-PSC_02</td>\n",
       "      <td>16594_293</td>\n",
       "      <td>53462</td>\n",
       "      <td>191.236364</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>6.991402</td>\n",
       "      <td>occlusion</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>PhC-C2DL-PSC_02</td>\n",
       "      <td>800_95</td>\n",
       "      <td>7221</td>\n",
       "      <td>29.886420</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>29.886420</td>\n",
       "      <td>exiting</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>PhC-C2DL-PSC_02</td>\n",
       "      <td>966_95</td>\n",
       "      <td>-1</td>\n",
       "      <td>107.200000</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>dying</td>\n",
       "      <td>errant annotation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>PhC-C2DL-PSC_02</td>\n",
       "      <td>901_115</td>\n",
       "      <td>9404</td>\n",
       "      <td>30.409302</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>30.409302</td>\n",
       "      <td>exiting</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>PhC-C2DL-PSC_02</td>\n",
       "      <td>904_0</td>\n",
       "      <td>59</td>\n",
       "      <td>62.409697</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>101 rows  9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             ds_name      gt_id  sol_id        cost  is_fn  is_correct  \\\n",
       "403  PhC-C2DL-PSC_02    695_209   24945   35.093333  False       False   \n",
       "404  PhC-C2DL-PSC_02    735_144   12848   33.972727  False       False   \n",
       "405  PhC-C2DL-PSC_02    744_161   15345   33.141573  False       False   \n",
       "406  PhC-C2DL-PSC_02     749_93    7037   30.245161  False        True   \n",
       "407  PhC-C2DL-PSC_02     764_19    1249   27.730000  False        True   \n",
       "..               ...        ...     ...         ...    ...         ...   \n",
       "499  PhC-C2DL-PSC_02  16594_293   53462  191.236364  False       False   \n",
       "500  PhC-C2DL-PSC_02     800_95    7221   29.886420  False        True   \n",
       "501  PhC-C2DL-PSC_02     966_95      -1  107.200000   True       False   \n",
       "502  PhC-C2DL-PSC_02    901_115    9404   30.409302  False        True   \n",
       "503  PhC-C2DL-PSC_02      904_0      59   62.409697  False       False   \n",
       "\n",
       "     real_cost     reason               note  \n",
       "403  33.022967    exiting                NaN  \n",
       "404  35.035729    exiting                NaN  \n",
       "405  22.597066    exiting                NaN  \n",
       "406  30.245161    exiting                NaN  \n",
       "407  27.730000    exiting                NaN  \n",
       "..         ...        ...                ...  \n",
       "499   6.991402  occlusion                NaN  \n",
       "500  29.886420    exiting                NaN  \n",
       "501  -1.000000      dying  errant annotation  \n",
       "502  30.409302    exiting                NaN  \n",
       "503   0.000000        NaN                NaN  \n",
       "\n",
       "[101 rows x 9 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_of_interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap_df.loc[503, 'reason'] = 'occlusion'\n",
    "ap_df.loc[503, 'note'] = 'migrating'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap_df.to_csv('/home/ddon0001/PhD/experiments/appearance_disappearance/disappearances.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trexp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
