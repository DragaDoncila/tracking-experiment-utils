{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yaml\n",
    "SOL_ROOT = '/home/ddon0001/PhD/experiments/scaled/no_div_constraint_err_seg'\n",
    "NO_MERGE_ROOT = '/home/ddon0001/PhD/experiments/scaled/no_merges_all'\n",
    "SCALES_PATH = '/home/ddon0001/PhD/data/cell_tracking_challenge/scales.yaml'\n",
    "\n",
    "ds_summary = pd.read_csv(f'{SOL_ROOT}/summary.csv')\n",
    "with open(SCALES_PATH, 'r') as f:\n",
    "    scales = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EDGE_FN = 'EdgeFlag.FALSE_NEG'\n",
    "EDGE_FP = 'EdgeFlag.FALSE_POS'\n",
    "EDGE_WS = 'EdgeFlag.WRONG_SEMANTIC'\n",
    "\n",
    "NODE_FN = 'NodeFlag.FALSE_NEG'\n",
    "NODE_FP = 'NodeFlag.FALSE_POS'\n",
    "NODE_NS = 'NodeFlag.NON_SPLIT'\n",
    "\n",
    "def has_adjacent_error_edges(sol, node):\n",
    "    if any(sol[edge][EDGE_FP] or sol[edge][EDGE_WS] for edge in sol.in_edges(node)):\n",
    "        return True\n",
    "    if any(sol[edge][EDGE_FP] or sol[edge][EDGE_WS] for edge in sol.out_edges(node)):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import json\n",
    "import os\n",
    "import networkx as nx\n",
    "\n",
    "total_fn = defaultdict(lambda : 0)\n",
    "total_edges = defaultdict(lambda : 0)\n",
    "all_edge_count = defaultdict(lambda : 0)\n",
    "ds_names = []\n",
    "us = []\n",
    "gt_us = []\n",
    "merge_vs = []\n",
    "true_successors = []\n",
    "correct_fate = []\n",
    "is_immediate_split = []\n",
    "\n",
    "count_should_divide = defaultdict(lambda : 0)\n",
    "for _, row in ds_summary.iterrows():\n",
    "    ds_name = row['ds_name']\n",
    "    gt_solution_path = os.path.join(SOL_ROOT, ds_name, 'matched_gt.graphml')\n",
    "    no_merge_gt_pth = os.path.join(NO_MERGE_ROOT, ds_name, 'matched_gt.graphml')\n",
    "    solution_path = os.path.join(SOL_ROOT, ds_name, 'matched_solution.graphml')\n",
    "    no_merge_sol_pth = os.path.join(NO_MERGE_ROOT, ds_name, 'matched_solution.graphml')\n",
    "    node_match_path = os.path.join(SOL_ROOT, ds_name, 'matching.json')\n",
    "    \n",
    "    \n",
    "    merge_gt = nx.read_graphml(gt_solution_path)\n",
    "    no_merge_gt = nx.read_graphml(no_merge_gt_pth)\n",
    "    merge_g = nx.read_graphml(solution_path, node_type=int)\n",
    "    no_merge_g = nx.read_graphml(no_merge_sol_pth, node_type=int)\n",
    "    with open(node_match_path) as f:\n",
    "        node_match = json.load(f)\n",
    "    gt_to_sol = {item[0]: item[1] for item in node_match}\n",
    "    sol_to_gt = {item[1]: item[0] for item in node_match}\n",
    "\n",
    "    total_fn[ds_name] = len([node for node in merge_gt.nodes if merge_gt.nodes[node][NODE_FN]])\n",
    "    all_edge_count[ds_name] = merge_gt.number_of_edges()\n",
    "    for merge_node in merge_g.nodes:\n",
    "        # we have a merge vertex\n",
    "        if merge_g.in_degree(merge_node) > 1:\n",
    "            parents = merge_g.predecessors(merge_node)\n",
    "            node_gt = sol_to_gt[merge_node]\n",
    "            for parent in parents:\n",
    "                # # should this parent be dividing?\n",
    "                # gt_parent = sol_to_gt[parent]\n",
    "                # if merge_gt.out_degree(gt_parent) > 1:\n",
    "                #     assert merge_g.out_degree(parent) <= 1\n",
    "                #     count_should_divide[ds_name] += 1\n",
    "                # continue\n",
    "                edge_of_interest = (parent, merge_node)\n",
    "                # this edge doesn't exist in gt\n",
    "                if merge_g.edges[edge_of_interest][EDGE_FP]:\n",
    "                    ds_names.append(ds_name)\n",
    "                    us.append(parent)\n",
    "                    merge_vs.append(merge_node)\n",
    "                    total_edges[ds_name] += 1\n",
    "                    if merge_g.out_degree(merge_node) > 1:\n",
    "                        is_immediate_split.append(True)\n",
    "                    else:\n",
    "                        is_immediate_split.append(False)\n",
    "                    \n",
    "                    # so what does gt_parent usually do?\n",
    "                    gt_parent = sol_to_gt[parent]\n",
    "                    gt_us.append(gt_parent)\n",
    "                    gt_kids = list(merge_gt.successors(gt_parent))\n",
    "                    n_kids = len(gt_kids)\n",
    "                    if n_kids == 0:\n",
    "                        # \"parent\" should be terminating\n",
    "                        true_successors.append('-4')\n",
    "                        correct_fate.append('terminate')\n",
    "                        continue\n",
    "                    elif n_kids == 1:\n",
    "                        kid = gt_kids[0]\n",
    "                        if merge_gt.nodes[kid]['t'] > merge_gt.nodes[gt_parent]['t'] + 1:\n",
    "                            # \"parent\" is frame skipping\n",
    "                            true_successors.append(kid)\n",
    "                            correct_fate.append('skip')\n",
    "                            continue\n",
    "                        else:\n",
    "                            # we're missing this vertex\n",
    "                            if merge_gt.nodes[kid][NODE_FN]:\n",
    "                                true_successors.append(kid)\n",
    "                                correct_fate.append('introduce')\n",
    "                                continue\n",
    "                            else:\n",
    "                                # \"parent\" should be migrating normally elsewhere\n",
    "                                true_successors.append(kid)\n",
    "                                correct_fate.append('migrate')\n",
    "                                continue\n",
    "                    raise ValueError(f'Parent {parent} fate missing!')\n",
    "                    #     # parent is splitting into two totally different children\n",
    "                    #     should_divide[ds_name].append(parent)\n",
    "merge_fate_df = pd.DataFrame({\n",
    "    'ds_name': ds_names,\n",
    "    'u': us,\n",
    "    'gt_u': gt_us,\n",
    "    'v': merge_vs,\n",
    "    'gt_vs': true_successors,\n",
    "    'correct_fate': correct_fate,\n",
    "    'is_immediate_split': is_immediate_split\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_fate_df['new_fate'] = 'Unknown'\n",
    "merge_fate_df['new_v'] = -1\n",
    "merge_fate_df['new_v_2'] = -1\n",
    "merge_fate_df['new_fate_correct'] = False\n",
    "for ds_name in merge_fate_df['ds_name'].unique():\n",
    "    original_merge_fps = merge_fate_df[merge_fate_df['ds_name'] == ds_name]\n",
    "\n",
    "    solution_path = os.path.join(SOL_ROOT, ds_name, 'matched_solution.graphml')\n",
    "    no_merge_sol_pth = os.path.join(NO_MERGE_ROOT, ds_name, 'matched_solution.graphml')\n",
    "    node_match_path = os.path.join(NO_MERGE_ROOT, ds_name, 'matching.json')\n",
    "    gt_solution_path = os.path.join(NO_MERGE_ROOT, ds_name, 'matched_gt.graphml')\n",
    "    \n",
    "    merge_g = nx.read_graphml(solution_path, node_type=int)\n",
    "    no_merge_g = nx.read_graphml(no_merge_sol_pth, node_type=int)\n",
    "    gt_g = nx.read_graphml(gt_solution_path)\n",
    "\n",
    "    with open(node_match_path) as f:\n",
    "        node_match = json.load(f)\n",
    "    gt_to_sol = {item[0]: item[1] for item in node_match}\n",
    "    sol_to_gt = {item[1]: item[0] for item in node_match}\n",
    "    for row in original_merge_fps.itertuples():\n",
    "        u = row.u\n",
    "        original_successors = list(merge_g.successors(u))\n",
    "        new_successors = list(no_merge_g.successors(u))\n",
    "        if len(new_successors) == 0:\n",
    "            merge_fate_df.loc[row.Index, 'new_fate'] = 'terminate'\n",
    "            merge_fate_df.loc[row.Index, 'new_v'] = -4\n",
    "            if row.correct_fate == 'terminate':\n",
    "                merge_fate_df.loc[row.Index, 'new_fate_correct'] = True\n",
    "        elif len(new_successors) == 1:\n",
    "            merge_fate_df.loc[row.Index, 'new_fate'] = 'migrate'\n",
    "            merge_fate_df.loc[row.Index, 'new_v'] = new_successors[0]\n",
    "            if row.correct_fate == 'migrate' and row.gt_vs == sol_to_gt[new_successors[0]]:\n",
    "                merge_fate_df.loc[row.Index, 'new_fate_correct'] = True\n",
    "        elif len(new_successors) == 2:\n",
    "            shared_successors = set(original_successors).intersection(set(new_successors))\n",
    "            if len(shared_successors) == 2:\n",
    "                raise ValueError(f'{ds_name} vertex {u} still has the same two successors.')\n",
    "            elif len(shared_successors) == 1:\n",
    "                # we share one successor and swapped the other one.\n",
    "                # is the shared successor the merge vertex?\n",
    "                if shared_successors.pop() == row.v:\n",
    "                    raise(ValueError(f'{ds_name} vertex {u} still flowing into merge vertex.'))\n",
    "                other_successor = list(set(original_successors) - shared_successors)[0]\n",
    "                merge_fate_df.loc[row.Index, 'new_fate'] = 'migrate'\n",
    "                merge_fate_df.loc[row.Index, 'new_v'] = other_successor\n",
    "            elif len(shared_successors) == 0:\n",
    "                # we weren't splitting before but now we are\n",
    "                if len(original_successors) == 1:\n",
    "                    merge_fate_df.loc[row.Index, 'new_fate'] = 'new_split'\n",
    "                # we were always splitting but now it's different\n",
    "                else:\n",
    "                    merge_fate_df.loc[row.Index, 'new_fate'] = 'different_split'\n",
    "                merge_fate_df.loc[row.Index, 'new_v'] = new_successors[0]\n",
    "                merge_fate_df.loc[row.Index, 'new_v_2'] = new_successors[1]\n",
    "        else:\n",
    "            raise ValueError(f'{ds_name} unexpected number of successors for vertex {u}: {len(new_successors)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_fate_df.groupby('correct_fate').new_fate.value_counts()\n",
    "# how many of those that **should** be migrating are actually part of division?\n",
    "# should we be checking if merge vertex is dividing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_immediate_split  correct_fate\n",
       "False               introduce       110\n",
       "                    terminate       105\n",
       "                    skip             23\n",
       "                    migrate          19\n",
       "True                introduce       276\n",
       "                    skip             22\n",
       "                    terminate        21\n",
       "                    migrate           6\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_fate_df.groupby('is_immediate_split').correct_fate.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_fate_df[merge_fate_df.new_fate_correct == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "merge_fate_df['old_cost'] = -1.0\n",
    "merge_fate_df['new_cost'] = -1.0\n",
    "merge_fate_df['old_area_prop'] = -1.0\n",
    "merge_fate_df['new_area_prop'] = -1.0\n",
    "merge_fate_df['old_chosen_neighbour_rank'] = -1\n",
    "merge_fate_df['new_chosen_neighbour_rank'] = -1\n",
    "merge_fate_df['old_sensitivity_diff'] = -1.0\n",
    "merge_fate_df['new_sensitivity_diff'] = -1.0\n",
    "\n",
    "# load merge edges with target and non merge edges with target\n",
    "all_edges_with_target_merge = pd.read_csv(f'{SOL_ROOT}/all_edges_with_target.csv')\n",
    "all_edges_with_target_no_merge = pd.read_csv(f'{NO_MERGE_ROOT}/all_edges_with_target.csv')\n",
    "for edge_row in merge_fate_df.itertuples():\n",
    "    # for each edge, assign the values of their features\n",
    "    merge_edge = all_edges_with_target_merge[(all_edges_with_target_merge.ds_name == edge_row.ds_name) & (all_edges_with_target_merge.u == edge_row.u) & (all_edges_with_target_merge.v == edge_row.v)]\n",
    "    if len(merge_edge) == 0:\n",
    "        print(\"MISSING MERGE EDGE\")\n",
    "    # assign into df\n",
    "    merge_fate_df.loc[edge_row.Index, 'old_cost'] = merge_edge.cost.values[0]\n",
    "    merge_fate_df.loc[edge_row.Index, 'old_area_prop'] = merge_edge.chosen_neighbour_area_prop.values[0]\n",
    "    merge_fate_df.loc[edge_row.Index, 'old_chosen_neighbour_rank'] = merge_edge.chosen_neighbour_rank.values[0]\n",
    "    merge_fate_df.loc[edge_row.Index, 'old_sensitivity_diff'] = merge_edge.sensitivity_diff.values[0]\n",
    "\n",
    "    if edge_row.new_v == -4:\n",
    "        # need to read the actual \"all edges file for this dataset\"\n",
    "        ds_all_edges = pd.read_csv(f'{SOL_ROOT}/{edge_row.ds_name}/all_edges.csv')\n",
    "        non_merge_edge = ds_all_edges[(ds_all_edges.u == edge_row.u) & (ds_all_edges.v == edge_row.new_v)]\n",
    "        merge_fate_df.loc[edge_row.Index, 'new_cost'] = non_merge_edge.cost.values[0]\n",
    "        merge_fate_df.loc[edge_row.Index, 'new_area_prop'] = 0\n",
    "        merge_fate_df.loc[edge_row.Index, 'new_sensitivity_diff'] = non_merge_edge.sensitivity_diff.values[0]\n",
    "        # need to rank the neighbour\n",
    "        all_neighbours_of_u_costs = ds_all_edges[(ds_all_edges.u == edge_row.u) & (ds_all_edges.v >= 0)].cost.values\n",
    "        all_neighbours_of_u_costs = np.append(all_neighbours_of_u_costs, non_merge_edge.cost.values[0])\n",
    "        merge_fate_df.loc[edge_row.Index, 'new_chosen_neighbour_rank'] = all_neighbours_of_u_costs.argsort().argsort()[-1]\n",
    "        continue \n",
    "\n",
    "    non_merge_edge = all_edges_with_target_no_merge[(all_edges_with_target_no_merge.ds_name == edge_row.ds_name) & (all_edges_with_target_no_merge.u == edge_row.u) & (all_edges_with_target_no_merge.v == edge_row.new_v)]\n",
    "    if len(non_merge_edge) == 0:\n",
    "        print(\"MISSING NON MERGE EDGE\")\n",
    "    merge_fate_df.loc[edge_row.Index, 'new_cost'] = non_merge_edge.cost.values[0]\n",
    "    merge_fate_df.loc[edge_row.Index, 'new_area_prop'] = non_merge_edge.chosen_neighbour_area_prop.values[0]\n",
    "    merge_fate_df.loc[edge_row.Index, 'new_chosen_neighbour_rank'] = non_merge_edge.chosen_neighbour_rank.values[0]\n",
    "    merge_fate_df.loc[edge_row.Index, 'new_sensitivity_diff'] = non_merge_edge.sensitivity_diff.values[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_fate_df.to_csv('/home/ddon0001/PhD/experiments/merge_resolution/merge_fates.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "plot_order = merge_fate_df.ds_name.value_counts().index\n",
    "merge_fate_df['chosen_rank_diff'] = merge_fate_df.new_chosen_neighbour_rank - merge_fate_df.old_chosen_neighbour_rank\n",
    "merge_fate_df['sensitivity_diff_diff'] = merge_fate_df.new_sensitivity_diff - merge_fate_df.old_sensitivity_diff\n",
    "merge_fate_df['area_prop_diff'] = merge_fate_df.new_area_prop - merge_fate_df.old_area_prop\n",
    "merge_fate_df['cost_diff'] = merge_fate_df.new_cost - merge_fate_df.old_cost\n",
    "\n",
    "grid = sns.FacetGrid(merge_fate_df, col='ds_name', col_order=plot_order, col_wrap=4, hue='new_fate_correct', sharex=False, sharey=False)\n",
    "# grid.map(sns.scatterplot, 'old_cost', 'new_cost')\n",
    "# grid.map(sns.scatterplot, 'old_area_prop', 'new_area_prop')\n",
    "# grid.map(sns.scatterplot, 'old_chosen_neighbour_rank', 'new_chosen_neighbour_rank')\n",
    "# grid.map(sns.scatterplot, 'old_sensitivity_diff', 'new_sensitivity_diff')\n",
    "\n",
    "grid.map(sns.countplot, 'chosen_rank_diff')\n",
    "# grid.map(sns.histplot, 'cost_diff')\n",
    "# grid.map(sns.histplot, 'area_prop_diff')\n",
    "# grid.map(sns.histplot, 'sensitivity_diff_diff')\n",
    "grid.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge_fate_df[merge_fate_df.chosen_rank_diff == 0].correct_fate.value_counts()\n",
    "merge_fate_df[merge_fate_df.v == merge_fate_df.new_v].correct_fate.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_fate_df[merge_fate_df.new_fate_correct == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many skip frames have we \"correctly\" captured through merge and split?\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import os\n",
    "import networkx as nx\n",
    "\n",
    "ds_names = []\n",
    "count_skip = []\n",
    "count_fn_adjacent = []\n",
    "count_correctly_merged = []\n",
    "for _, row in ds_summary.iterrows():\n",
    "    ds_name = row['ds_name']\n",
    "    gt_solution_path = os.path.join(SOL_ROOT, ds_name, 'matched_gt.graphml')\n",
    "    solution_path = os.path.join(SOL_ROOT, ds_name, 'matched_solution.graphml')\n",
    "    node_match_path = os.path.join(SOL_ROOT, ds_name, 'matching.json')\n",
    "    \n",
    "    merge_gt = nx.read_graphml(gt_solution_path)\n",
    "    merge_g = nx.read_graphml(solution_path, node_type=int)\n",
    "\n",
    "    with open(node_match_path) as f:\n",
    "        node_match = json.load(f)\n",
    "    gt_to_sol = {item[0]: item[1] for item in node_match}\n",
    "    sol_to_gt = {item[1]: item[0] for item in node_match}\n",
    "\n",
    "    skip_edges = [edge for edge in merge_gt.edges if merge_gt.nodes[edge[0]]['t'] < merge_gt.nodes[edge[1]]['t'] - 1]\n",
    "    count_correct = 0\n",
    "    count_fn = 0\n",
    "    for edge in skip_edges:\n",
    "        if edge[0] not in gt_to_sol or edge[1] not in gt_to_sol:\n",
    "            count_fn += 1\n",
    "            continue\n",
    "        sol_parent = gt_to_sol[edge[0]]\n",
    "        sol_child = gt_to_sol[edge[1]]\n",
    "        # we want parent to be flowing into a merge\n",
    "        if merge_g.out_degree(sol_parent) == 1:\n",
    "            successor = list(merge_g.successors(sol_parent))[0]\n",
    "            if merge_g.in_degree(successor) > 1:\n",
    "                # sol_parent is flowing into a merge - follow the merge to its end\n",
    "                v = successor\n",
    "                children = list(merge_g.successors(v))\n",
    "                while len(children) == 1:\n",
    "                    v = children[0]\n",
    "                    children = list(merge_g.successors(v))\n",
    "                # we've either split or we've terminated\n",
    "                if len(children) > 1:\n",
    "                    # we've split - are any of the children sol_child?\n",
    "                    if any(child == sol_child for child in children):\n",
    "                        count_correct += 1\n",
    "    ds_names.append(ds_name)\n",
    "    count_correctly_merged.append(count_correct)\n",
    "    count_skip.append(len(skip_edges))\n",
    "    count_fn_adjacent.append(count_fn)\n",
    "skip_merges = pd.DataFrame({\n",
    "    'ds_name': ds_names,\n",
    "    'total_skips': count_skip,\n",
    "    'fn_adjacent': count_fn_adjacent,\n",
    "    'total_correct': count_correctly_merged\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_merges.to_csv(os.path.join(SOL_ROOT, 'skip_merges.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many merges correctly fuel divisions?\n",
    "\n",
    "ds_names = []\n",
    "total_merges = []\n",
    "# looking at the fate of the merge now\n",
    "count_correct_division = []\n",
    "count_incorrect_division = []\n",
    "count_terminating_correct = []\n",
    "count_terminating_incorrect = []\n",
    "total_explained = []\n",
    "for _, row in ds_summary.iterrows():\n",
    "    ds_name = row['ds_name']\n",
    "    gt_solution_path = os.path.join(SOL_ROOT, ds_name, 'matched_gt.graphml')\n",
    "    solution_path = os.path.join(SOL_ROOT, ds_name, 'matched_solution.graphml')\n",
    "    node_match_path = os.path.join(SOL_ROOT, ds_name, 'matching.json')\n",
    "    \n",
    "    merge_gt = nx.read_graphml(gt_solution_path)\n",
    "    merge_g = nx.read_graphml(solution_path, node_type=int)\n",
    "\n",
    "    with open(node_match_path) as f:\n",
    "        node_match = json.load(f)\n",
    "    gt_to_sol = {item[0]: item[1] for item in node_match}\n",
    "    sol_to_gt = {item[1]: item[0] for item in node_match}\n",
    "\n",
    "    merges = [node for node in merge_g.nodes if merge_g.in_degree(node) > 1]\n",
    "    total = len(merges)\n",
    "    n_correct = 0\n",
    "    n_incorrect = 0\n",
    "    n_terminating_correct = 0\n",
    "    n_terminating_incorrect = 0\n",
    "\n",
    "    for merge_node in merges:\n",
    "        children = list(merge_g.successors(merge_node))\n",
    "        v = merge_node\n",
    "        while len(children) == 1:\n",
    "            v = children[0]\n",
    "            children = list(merge_g.successors(v))\n",
    "        if len(children) == 0:\n",
    "            gt_term = sol_to_gt[v]\n",
    "            # gt node also terminates\n",
    "            if len(list(merge_gt.successors(gt_term))) == 0:\n",
    "                n_terminating_correct += 1\n",
    "            else:\n",
    "                n_terminating_incorrect += 1\n",
    "        elif len(children) > 1:\n",
    "            div_edges = [(v, child) for child in children]\n",
    "            # are any edges wrong?\n",
    "            if any(merge_g.edges[edge][EDGE_FP] or merge_g.edges[edge][EDGE_WS] for edge in div_edges):\n",
    "                n_incorrect += 1\n",
    "            else:\n",
    "                n_correct += 1\n",
    "    ds_names.append(ds_name)\n",
    "    total_merges.append(total)\n",
    "    count_correct_division.append(n_correct)\n",
    "    count_incorrect_division.append(n_incorrect)\n",
    "    count_terminating_correct.append(n_terminating_correct)\n",
    "    count_terminating_incorrect.append(n_terminating_incorrect)\n",
    "    total_explained.append(n_correct + n_incorrect + n_terminating_correct + n_terminating_incorrect)\n",
    "merge_fates = pd.DataFrame({\n",
    "    'ds_name': ds_names,\n",
    "    'total_merges': total_merges,\n",
    "    'total_explained': total_explained,\n",
    "    'correct_division': count_correct_division,\n",
    "    'incorrect_division': count_incorrect_division,\n",
    "    'terminating_correct': count_terminating_correct,\n",
    "    'terminating_incorrect': count_terminating_incorrect,\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_fates[merge_fates['total_merges'] > 0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# symmetry/asymmetry of divisions \n",
    "import numpy as np\n",
    "\n",
    "\n",
    "div_diff_dict = {\n",
    "    'ds_name': [],\n",
    "    'div_parent_index': [],\n",
    "    'is_correct': [],\n",
    "    'is_superparent': [],\n",
    "    'child_distance_1': [],\n",
    "    'child_distance_2': [],\n",
    "    'child_distance_difference': [],\n",
    "    'interchild_distance': [],\n",
    "    'div_angle': []\n",
    "}\n",
    "\n",
    "# WE TAKE TWO CLOSEST CHILDREN\n",
    "for _, row in ds_summary.iterrows():\n",
    "    ds_name = row['ds_name']\n",
    "    solution_path = os.path.join(SOL_ROOT, ds_name, 'matched_solution.graphml')\n",
    "    \n",
    "    merge_g = nx.read_graphml(solution_path, node_type=int)\n",
    "    location_cols = ['z', 'y', 'x'] if 'z' in merge_g.nodes[0] else ['y', 'x']\n",
    "    scale = scales[ds_name]['pixel_scale']\n",
    "\n",
    "    divs = [node for node in merge_g.nodes if merge_g.out_degree(node) > 1]\n",
    "    for div in divs:\n",
    "        children = list(merge_g.successors(div))\n",
    "        is_superparent = len(children) > 2\n",
    "        is_correct = not any(merge_g.edges[(div, child)][EDGE_FP] or merge_g.edges[(div, child)][EDGE_WS] for child in children)\n",
    "        parent_coords = np.asarray([merge_g.nodes[div][col] * scale[i] for i, col in enumerate(location_cols)])\n",
    "        child_coords = [np.asarray([merge_g.nodes[child][col] * scale[i] for i, col in enumerate(location_cols)]) for child in children]\n",
    "        child_distances = [np.linalg.norm(parent_coords - child) for child in child_coords]\n",
    "        dist_indices = np.argsort(child_distances)\n",
    "        closest_children = [child_coords[i] for i in dist_indices[:2]]\n",
    "        distances = [child_distances[i] for i in dist_indices[:2]]\n",
    "        div_angle = np.degrees(np.arccos(\n",
    "            np.dot(closest_children[0] - parent_coords, closest_children[1] - parent_coords) /\n",
    "            (distances[0] * distances[1])\n",
    "        ))\n",
    "    \n",
    "        div_diff_dict['ds_name'].append(ds_name)\n",
    "        div_diff_dict['div_parent_index'].append(div)\n",
    "        div_diff_dict['is_correct'].append(is_correct)\n",
    "        div_diff_dict['is_superparent'].append(is_superparent)\n",
    "        div_diff_dict['child_distance_1'].append(distances[0])\n",
    "        div_diff_dict['child_distance_2'].append(distances[1])\n",
    "        div_diff_dict['child_distance_difference'].append(distances[1] - distances[0])\n",
    "        div_diff_dict['interchild_distance'].append(np.linalg.norm(closest_children[0] - closest_children[1]))\n",
    "        div_diff_dict['div_angle'].append(div_angle)\n",
    "\n",
    "div_diff_df = pd.DataFrame(div_diff_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "grid = sns.FacetGrid(div_diff_df, col='ds_name', col_wrap=4, sharex=False, sharey=False, legend_out=True, hue='is_correct')\n",
    "grid.map_dataframe(sns.histplot, x='interchild_distance')\n",
    "grid.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "grid = sns.FacetGrid(div_diff_df, col='ds_name', col_wrap=4, sharex=False, sharey=False, legend_out=True, hue='is_correct')\n",
    "grid.map_dataframe(sns.histplot, x='child_distance_difference')\n",
    "grid.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "grid = sns.FacetGrid(div_diff_df, col='ds_name', col_wrap=4, sharex=False, sharey=False, legend_out=True, hue='is_correct')\n",
    "grid.map_dataframe(sns.histplot, x='div_angle')\n",
    "grid.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "grid = sns.FacetGrid(div_diff_df, col='ds_name', col_wrap=4, sharex=True, sharey=False, legend_out=True, hue='is_correct')\n",
    "grid.map_dataframe(sns.scatterplot, x='div_angle', y='interchild_distance', alpha=0.4)\n",
    "grid.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "grid = sns.FacetGrid(div_diff_df, col='ds_name', col_wrap=4, sharex=True, sharey=False, legend_out=True, hue='is_correct')\n",
    "grid.map_dataframe(sns.scatterplot, x='div_angle', y='child_distance_difference', alpha=0.4)\n",
    "grid.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import networkx as nx\n",
    "\n",
    "# which kth neighbour is chosen for correct migration/incorrect migration\n",
    "# which kth neighbours are chosen for correct division vs. incorrect division\n",
    "kth_neighbour_dict = {\n",
    "    'ds_name': [],\n",
    "    'source_v': [],\n",
    "    'dest_v': [],\n",
    "    'edge_correct': [],\n",
    "    'neighbour_rank': []\n",
    "}\n",
    "for _, row in ds_summary.iterrows():\n",
    "    ds_name = row['ds_name']\n",
    "    solution_path = os.path.join(SOL_ROOT, ds_name, 'matched_solution.graphml')\n",
    "    all_edges_path = os.path.join(SOL_ROOT, ds_name, 'all_edges.csv')\n",
    "    \n",
    "    merge_gt = nx.read_graphml(solution_path, node_type=int)\n",
    "    all_edges = pd.read_csv(all_edges_path)\n",
    "    location_cols = ['z', 'y', 'x'] if 'z' in merge_gt.nodes[0] else ['y', 'x']\n",
    "    scale = scales[ds_name]['pixel_scale']\n",
    "\n",
    "    for edge in merge_gt.edges:\n",
    "        source = edge[0]\n",
    "        dest = edge[1]\n",
    "        edge_correct = not (merge_gt.edges[edge][EDGE_FP] or merge_gt.edges[edge][EDGE_WS])\n",
    "\n",
    "        other_edges = all_edges[(all_edges['u'] == source) & (all_edges['v'] >= 0)].sort_values(by='distance').reset_index()\n",
    "        neighbour_rank = other_edges[other_edges['v'] == dest].index[0]\n",
    "        kth_neighbour_dict['ds_name'].append(ds_name)\n",
    "        kth_neighbour_dict['source_v'].append(source)\n",
    "        kth_neighbour_dict['dest_v'].append(dest)\n",
    "        kth_neighbour_dict['edge_correct'].append(edge_correct)\n",
    "        kth_neighbour_dict['neighbour_rank'].append(neighbour_rank)\n",
    "kth_neighbour_df = pd.DataFrame(kth_neighbour_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "grid = sns.FacetGrid(kth_neighbour_df, col='ds_name', col_wrap=4, sharex=False, sharey=False, legend_out=True, hue='edge_correct')\n",
    "grid.map_dataframe(sns.countplot, x='neighbour_rank')\n",
    "grid.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "no_zeros = kth_neighbour_df[kth_neighbour_df['neighbour_rank'] > 0]\n",
    "grid = sns.FacetGrid(no_zeros, col='ds_name', col_wrap=4, sharex=False, sharey=False, legend_out=True)\n",
    "grid.map_dataframe(sns.countplot, x='neighbour_rank', hue='edge_correct')\n",
    "grid.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gt neighbour rank\n",
    "import os\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "GT_ROOT =  '/home/ddon0001/PhD/experiments/scaled/gt_no_div_constraint'\n",
    "kth_neighbour_dict = {\n",
    "    'ds_name': [],\n",
    "    'source_v': [],\n",
    "    'dest_v': [],\n",
    "    'neighbour_rank': []\n",
    "}\n",
    "for _, row in ds_summary.iterrows():\n",
    "    ds_name = row['ds_name']\n",
    "    solution_path = os.path.join(GT_ROOT, ds_name, 'matched_gt.graphml')\n",
    "    matching_path = os.path.join(GT_ROOT, ds_name, 'matching.json')\n",
    "    all_edges_path = os.path.join(GT_ROOT, ds_name, 'all_edges.csv')\n",
    "    \n",
    "    merge_gt = nx.read_graphml(solution_path)\n",
    "    all_edges = pd.read_csv(all_edges_path)\n",
    "    location_cols = ['x', 'y', 'z'] if 'z' in merge_gt.nodes[list(merge_gt.nodes)[0]] else ['x', 'y']\n",
    "\n",
    "    with open(matching_path) as f:\n",
    "        node_match = json.load(f)\n",
    "    gt_to_sol = {item[0]: item[1] for item in node_match}\n",
    "    sol_to_gt = {item[1]: item[0] for item in node_match}\n",
    "\n",
    "    for edge in merge_gt.edges:\n",
    "        source = edge[0]\n",
    "        dest = edge[1]\n",
    "\n",
    "        sol_source = gt_to_sol[source]\n",
    "        sol_dest = gt_to_sol[dest]\n",
    "        other_edges = all_edges[(all_edges['u'] == sol_source) & (all_edges['v'] >= 0)].sort_values(by='distance').reset_index()\n",
    "        v_edge = other_edges[other_edges['v'] == sol_dest]\n",
    "        if v_edge.empty:\n",
    "            neighbour_rank = -1\n",
    "        else:\n",
    "            neighbour_rank = v_edge.index[0]\n",
    "        kth_neighbour_dict['ds_name'].append(ds_name)\n",
    "        kth_neighbour_dict['source_v'].append(sol_source)\n",
    "        kth_neighbour_dict['dest_v'].append(sol_dest)\n",
    "        kth_neighbour_dict['neighbour_rank'].append(neighbour_rank)\n",
    "gt_neighbour_rank_df = pd.DataFrame(kth_neighbour_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "grid = sns.FacetGrid(gt_neighbour_rank_df[gt_neighbour_rank_df['neighbour_rank'] >= 0], col='ds_name', col_wrap=4, sharex=False, sharey=False, legend_out=True)\n",
    "grid.map_dataframe(sns.countplot, x='neighbour_rank')\n",
    "grid.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "grid = sns.FacetGrid(gt_neighbour_rank_df[gt_neighbour_rank_df['neighbour_rank'] > 0], col='ds_name', col_wrap=4, sharex=False, sharey=False, legend_out=True)\n",
    "grid.map_dataframe(sns.countplot, x='neighbour_rank')\n",
    "grid.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# investigating division costs as prop. of migration cost\n",
    "\n",
    "# cost of correct vs. incorrect edges for divisions vs. migration\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import os\n",
    "from tracktour._tracker import VirtualVertices\n",
    "\n",
    "chosen_cost_dict = {\n",
    "    'ds_name': [],\n",
    "    'div_parent_index': [],\n",
    "    'is_superparent': [],\n",
    "    'modified_cost': [],\n",
    "    'division_cost': [],\n",
    "    'chosen_children_dist': [],\n",
    "    # 'cheating_false', 'cheating_true', 'valid_false', 'valid_true'\n",
    "    'category': [],\n",
    "}\n",
    "for _, row in ds_summary.iterrows():\n",
    "    ds_name = row['ds_name']\n",
    "    solution_path = os.path.join(SOL_ROOT, ds_name, 'matched_solution.graphml')\n",
    "    all_edges_path = os.path.join(SOL_ROOT, ds_name, 'all_edges.csv')\n",
    "\n",
    "    merge_g = nx.read_graphml(solution_path, node_type=int)\n",
    "    all_edges_path = os.path.join(SOL_ROOT, ds_name, 'all_edges.csv')\n",
    "    location_cols = ['z', 'y', 'x'] if 'z' in merge_g.nodes[0] else ['y', 'x']\n",
    "    scale = scales[ds_name]['pixel_scale']\n",
    "\n",
    "    divs = [node for node in merge_g.nodes if merge_g.out_degree(node) > 1]\n",
    "    for div in divs:\n",
    "        children = list(merge_g.successors(div))\n",
    "        is_superparent = len(children) > 2\n",
    "        is_correct = not any(merge_g.edges[(div, child)][EDGE_FP] or merge_g.edges[(div, child)][EDGE_WS] for child in children)\n",
    "        parent_coords = np.asarray([merge_g.nodes[div][col] * scale[i] for i, col in enumerate(location_cols)])\n",
    "        child_coords = [np.asarray([merge_g.nodes[child][col] * scale[i] for i, col in enumerate(location_cols)]) for child in children]\n",
    "        distances = np.sort([np.linalg.norm(parent_coords - child) for child in child_coords])\n",
    "\n",
    "        closest_children_dist = distances[0] + distances[1]\n",
    "        div_edge = all_edges[(all_edges['u'] == VirtualVertices.DIV.value) & (all_edges['v'] == div)]\n",
    "        div_cost = div_edge['cost'].values[0]\n",
    "        is_valid = div_edge['flow'].values[0] > 0\n",
    "\n",
    "        if is_valid:\n",
    "            if is_correct:\n",
    "                category = 'valid_true'\n",
    "            else:\n",
    "                category = 'valid_false'\n",
    "        else:\n",
    "            if is_correct:\n",
    "                category = 'cheating_true'\n",
    "            else:\n",
    "                category = 'cheating_false'\n",
    "\n",
    "        chosen_cost_dict['ds_name'].append(ds_name)\n",
    "        chosen_cost_dict['div_parent_index'].append(div)\n",
    "        chosen_cost_dict['is_superparent'].append(is_superparent)\n",
    "        # what division cost was incorrectly not chosen? i.e. the false div cost of merged divs?\n",
    "        chosen_cost_dict['modified_cost'].append(div_cost if is_valid else closest_children_dist)\n",
    "        chosen_cost_dict['division_cost'].append(div_cost)\n",
    "        chosen_cost_dict['chosen_children_dist'].append(closest_children_dist)\n",
    "        chosen_cost_dict['category'].append(category)\n",
    "\n",
    "chosen_cost_df = pd.DataFrame(chosen_cost_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "grid = sns.FacetGrid(chosen_cost_df, col='ds_name', col_wrap=4, sharex=False, sharey=False, legend_out=True, hue='category')\n",
    "grid.map_dataframe(sns.scatterplot, x='modified_cost', y='chosen_children_dist')\n",
    "grid.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "grid = sns.FacetGrid(chosen_cost_df, col='ds_name', col_wrap=4, sharex=False, sharey=False, legend_out=True, hue='category')\n",
    "grid.map_dataframe(sns.scatterplot, x='division_cost', y='chosen_children_dist', alpha=0.4)\n",
    "grid.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "chosen_cost_df['is_correct'] = chosen_cost_df.category.str.contains('true')\n",
    "chosen_cost_df['is_valid'] = chosen_cost_df.category.str.contains('valid')\n",
    "grid = sns.FacetGrid(chosen_cost_df, col='ds_name', col_wrap=4, sharex=False, sharey=False, legend_out=True, hue='is_correct')\n",
    "grid.map_dataframe(sns.scatterplot, x='division_cost', y='chosen_children_dist', alpha=0.4)\n",
    "grid.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = sns.FacetGrid(chosen_cost_df, col='ds_name', col_wrap=4, sharex=False, sharey=False, legend_out=True, hue='is_valid')\n",
    "grid.map_dataframe(sns.scatterplot, x='division_cost', y='chosen_children_dist', alpha=0.3)\n",
    "grid.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solution_path = os.path.join(SOL_ROOT, 'Fluo-N3DH-CE_02', 'matched_solution.graphml')\n",
    "all_edges_path = os.path.join(SOL_ROOT, 'Fluo-N3DH-CE_02', 'all_edges.csv')\n",
    "\n",
    "merge_g = nx.read_graphml(solution_path, node_type=int)\n",
    "all_edges = pd.read_csv(all_edges_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# structural property of tracks - mergeless tracks\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import networkx as nx\n",
    "\n",
    "all_components = dict()\n",
    "for _, row in ds_summary.iterrows():\n",
    "    ds_name = row['ds_name']\n",
    "    solution_path = os.path.join(SOL_ROOT, ds_name, 'matched_solution.graphml')\n",
    "    all_edges_path = os.path.join(SOL_ROOT, ds_name, 'all_edges.csv')\n",
    "\n",
    "    merge_g = nx.read_graphml(solution_path, node_type=int)\n",
    "    all_edges = pd.read_csv(all_edges_path)\n",
    "\n",
    "    undirected_g = merge_g.to_undirected()\n",
    "    components = defaultdict(dict)\n",
    "    for i, component in enumerate(nx.connected_components(undirected_g)):\n",
    "        components[i]['nodes'] = component\n",
    "        components[i]['edges'] = list(merge_g.subgraph(component).edges)\n",
    "        components[i]['total_edges'] = len(components[i]['edges'])\n",
    "        components[i]['count_nodes'] = len(component)\n",
    "\n",
    "        has_merges = [merge_g.in_degree(node) > 1 for node in component]\n",
    "        count_merges = sum(has_merges)\n",
    "        components[i]['has_merge'] = count_merges > 0\n",
    "        components[i]['count_merges'] = count_merges\n",
    "\n",
    "        has_divisions = [merge_g.out_degree(node) > 1 for node in component]\n",
    "        count_divisions = sum(has_divisions)\n",
    "        components[i]['has_division'] = count_divisions > 0\n",
    "        components[i]['count_divisions'] = count_divisions\n",
    "\n",
    "        wrong_edge = [(merge_g.edges[edge][EDGE_FP] or merge_g.edges[edge][EDGE_WS]) for edge in components[i]['edges']]\n",
    "        components[i]['count_wrong_edges'] = wrong_edge.count(True)\n",
    "        components[i]['count_correct_edges'] = wrong_edge.count(False)\n",
    "        components[i]['is_correct'] = components[i]['count_correct_edges'] == components[i]['total_edges']\n",
    "    all_components[ds_name] = components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_name, component_id, has_merge, is_fully_correct, prop_correct_edges\n",
    "ds_names = []\n",
    "comp_ids = []\n",
    "has_merge = []\n",
    "count_merge = []\n",
    "has_div = []\n",
    "count_div = []\n",
    "is_fully_correct = []\n",
    "prop_correct_edges = []\n",
    "total_edges = []\n",
    "for ds_name, comp_info in all_components.items():\n",
    "    for comp_id, comp in comp_info.items():\n",
    "        ds_names.append(ds_name)\n",
    "        comp_ids.append(comp_id)\n",
    "        has_merge.append(comp['has_merge'])\n",
    "        count_merge.append(comp['count_merges'])\n",
    "        has_div.append(comp['has_division'])\n",
    "        count_div.append(comp['count_divisions'])\n",
    "        is_fully_correct.append(comp['is_correct'])\n",
    "        if comp['total_edges']> 0:\n",
    "            prop_correct_edges.append(comp['count_correct_edges']/comp['total_edges'])\n",
    "        else:\n",
    "            prop_correct_edges.append(0)\n",
    "        total_edges.append(comp['total_edges'])\n",
    "component_df = pd.DataFrame({\n",
    "    'ds_name': ds_names,\n",
    "    'component': comp_ids,\n",
    "    'has_merge': has_merge,\n",
    "    'count_merges': count_merge,\n",
    "    'has_div': has_div,\n",
    "    'count_div': count_div,\n",
    "    'is_correct': is_fully_correct,\n",
    "    'prop_correct': prop_correct_edges,\n",
    "    'total_edges': total_edges\n",
    "})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from tracktour._tracker import VirtualVertices\n",
    "\n",
    "ds_names = []\n",
    "node_ids = []\n",
    "node_areas = []\n",
    "comp_ids = []\n",
    "# true if component contains a merge\n",
    "merge_comp = []\n",
    "# true if component contains a div\n",
    "div_comp = []\n",
    "# true if component is entirely correct\n",
    "is_correct_comp = []\n",
    "# true if node is a parent of a division\n",
    "is_parent = []\n",
    "# true if node is a parent of 3+ children\n",
    "is_superparent = []\n",
    "# true if the node has two incoming edges\n",
    "is_merge_vertex = []\n",
    "# -1 if node is not a parent, otherwise difference in distance to (two closest) children\n",
    "child_distance_difference = []\n",
    "# -1 if node is not a parent, otherwise sum of distances to (two closest) children\n",
    "child_distance_sum = []\n",
    "# -1 if node is not a parent, otherwise distance between two closest children\n",
    "interchild_distance = []\n",
    "# -1 if node is not a parent, otherwise angle between two closest children\n",
    "# arcos of dot product of vectors from parent to children scaled by product of magnitudes\n",
    "div_angle = []\n",
    "# True if node is parent and both children are correct, otherwise False\n",
    "div_correct = []\n",
    "# True if node is parent and there is division flow into node\n",
    "div_valid = []\n",
    "# -1 if node has no successors, otherwise distance rank of the first child\n",
    "first_chosen_neighbour_rank = []\n",
    "# -1 if node is not a parent, otherwise distance rank of the second child\n",
    "second_chosen_neighbour_rank = []\n",
    "# -1 if node has no successors, otherwise distance to the closest child\n",
    "first_child_distance = []\n",
    "# -1 if node is not a parent, otherwise distance to the second closest child\n",
    "second_child_distance = []\n",
    "# -1 if node has no successors, otherwise area of the first child\n",
    "first_child_area = []\n",
    "# -1 if node is not a parent, otherwise area of the second child\n",
    "second_child_area = []\n",
    "# sum of areas of two children if node is parent, otherwise -1\n",
    "child_area_sum = []\n",
    "# cost of the Dv edge for node v (regardless of whether it was used)\n",
    "div_cost = []\n",
    "# True if node is not a parent, and the edge to its child is correct, otherwise False\n",
    "mig_correct = []\n",
    "for _, row in ds_summary.iterrows():\n",
    "    ds_name = row['ds_name']\n",
    "    det_path = row['det_path']\n",
    "    solution_path = os.path.join(SOL_ROOT, ds_name, 'matched_solution.graphml')\n",
    "    all_edges_path = os.path.join(SOL_ROOT, ds_name, 'all_edges.csv')\n",
    "\n",
    "    merge_g = nx.read_graphml(solution_path, node_type=int)\n",
    "    all_edges = pd.read_csv(all_edges_path)\n",
    "    det_df = pd.read_csv(det_path)\n",
    "    location_cols = ['z', 'y', 'x'] if 'z' in merge_g.nodes[0] else ['y', 'x']\n",
    "    scale = scales[ds_name]['pixel_scale']\n",
    "\n",
    "    ds_components = all_components[ds_name]\n",
    "    for comp_id, comp in ds_components.items():\n",
    "        for node in comp['nodes']:\n",
    "            ds_names.append(ds_name)\n",
    "            node_ids.append(node)\n",
    "            node_areas.append(det_df.loc[node].area)\n",
    "            comp_ids.append(comp_id)\n",
    "            merge_comp.append(comp['has_merge'])\n",
    "            div_comp.append(comp['has_division'])\n",
    "            is_correct_comp.append(comp['is_correct'])\n",
    "            is_parent.append(merge_g.out_degree(node) > 1)\n",
    "            is_superparent.append(merge_g.out_degree(node) > 2)\n",
    "            is_merge_vertex.append(merge_g.in_degree(node) > 1)\n",
    "\n",
    "            children = list(merge_g.successors(node))\n",
    "            parent_coords = np.asarray([merge_g.nodes[node][col] * scale[i] for i, col in enumerate(location_cols)])\n",
    "            child_coords = [np.asarray([merge_g.nodes[child][col] * scale[i] for i, col in enumerate(location_cols)]) for child in children]\n",
    "            child_distances = [np.linalg.norm(parent_coords - child) for child in child_coords]\n",
    "            dist_indices = np.argsort(child_distances)\n",
    "            children = [children[i] for i in dist_indices]\n",
    "            closest_children = [child_coords[i] for i in dist_indices]\n",
    "            distances = [child_distances[i] for i in dist_indices]\n",
    "\n",
    "            child_dist_diff = -1\n",
    "            child_dist_sum = -1\n",
    "            ichild_dist = -1\n",
    "            ang = -1\n",
    "            first_neighb = -1\n",
    "            second_neighb = -1\n",
    "            first_dist = -1\n",
    "            second_dist = -1\n",
    "            first_area = -1\n",
    "            second_area = -1\n",
    "            area_sum = -1\n",
    "            div_cos = -1\n",
    "            div_corr = False\n",
    "            div_val = False\n",
    "            mig_corr = False\n",
    "            if len(children) > 0:\n",
    "                # save first child stuff\n",
    "                other_edges = all_edges[(all_edges['u'] == node) & (all_edges['v'] >= 0)].sort_values(by='distance').reset_index()\n",
    "                neighbour_ranks = [other_edges[other_edges['v'] == dest].index[0] for dest in children]\n",
    "                first_neighb = neighbour_ranks[0]\n",
    "                first_dist = distances[0]\n",
    "                first_area = det_df.loc[children[0]].area\n",
    "                if len(children) == 1:\n",
    "                    mig_corr = (not (merge_g.edges[(node, children[0])][EDGE_FP] or merge_g.edges[(node, children[0])][EDGE_WS]))\n",
    "                else:\n",
    "                    ang = np.degrees(np.arccos(\n",
    "                        np.dot(closest_children[0] - parent_coords, closest_children[1] - parent_coords) /\n",
    "                        (distances[0] * distances[1])\n",
    "                    ))\n",
    "                    div_corr = not any(merge_g.edges[(node, child)][EDGE_FP] or merge_g.edges[(node, child)][EDGE_WS] for child in children)\n",
    "                    second_neighb = neighbour_ranks[1]\n",
    "                    second_dist = distances[1]\n",
    "                    second_area = det_df.loc[children[1]].area\n",
    "                    area_sum = first_area + second_area\n",
    "                    child_dist_diff = distances[1] - distances[0]\n",
    "                    child_dist_sum = distances[1] + distances[0]\n",
    "                    ichild_dist = np.linalg.norm(closest_children[0] - closest_children[1])\n",
    "                    div_edge = all_edges[(all_edges['u'] == VirtualVertices.DIV.value) & (all_edges['v'] == node)]\n",
    "                    if len(div_edge):\n",
    "                        div_cos = div_edge['cost'].values[0]\n",
    "                        div_val = div_edge['flow'].values[0] > 0\n",
    "            child_distance_difference.append(child_dist_diff)\n",
    "            child_distance_sum.append(child_dist_sum)\n",
    "            interchild_distance.append(ichild_dist)\n",
    "            div_angle.append(ang)\n",
    "            child_area_sum.append(area_sum)\n",
    "            first_chosen_neighbour_rank.append(first_neighb)\n",
    "            second_chosen_neighbour_rank.append(second_neighb)\n",
    "            first_child_distance.append(first_dist)\n",
    "            first_child_area.append(first_area)\n",
    "            second_child_distance.append(second_dist)\n",
    "            second_child_area.append(second_area)\n",
    "            div_cost.append(div_cos)\n",
    "            div_correct.append(div_corr)\n",
    "            div_valid.append(div_val)\n",
    "            mig_correct.append(mig_corr)\n",
    "all_node_info = pd.DataFrame({\n",
    "    'ds_name': ds_names,\n",
    "    'node_id': node_ids,\n",
    "    'node_area': node_areas,\n",
    "    'component_id': comp_ids,\n",
    "    'comp_merge': merge_comp,\n",
    "    'comp_div': div_comp,\n",
    "    'comp_correct': is_correct_comp,\n",
    "    'is_parent': is_parent,\n",
    "    'is_superparent': is_superparent,\n",
    "    'is_merge_vertex': is_merge_vertex,\n",
    "    'child_distance_difference': child_distance_difference,\n",
    "    'child_distance_sum': child_distance_sum,\n",
    "    'interchild_distance': interchild_distance,\n",
    "    'div_angle': div_angle,\n",
    "    'child_area_sum': child_area_sum,\n",
    "    'div_correct': div_correct,\n",
    "    'div_valid': div_valid,\n",
    "    'first_chosen_neighbour_rank': first_chosen_neighbour_rank,\n",
    "    'second_chosen_neighbour_rank': second_chosen_neighbour_rank,\n",
    "    'first_child_distance': first_child_distance,\n",
    "    'first_child_area': first_child_area,\n",
    "    'second_child_distance': second_child_distance,\n",
    "    'second_child_area': second_child_area,\n",
    "    'div_cost': div_cost,\n",
    "    'mig_correct': mig_correct\n",
    "})\n",
    "# all_node_info.to_csv(os.path.join(SOL_ROOT, 'all_node_info.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "grid = sns.FacetGrid(component_df, col='ds_name', col_wrap=4, sharex=True, sharey=False, legend_out=True, hue='has_merge')\n",
    "grid.map_dataframe(sns.countplot, x='is_correct')\n",
    "grid.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "grid = sns.FacetGrid(component_df[component_df.has_merge == False], col='ds_name', col_wrap=4, sharex=True, sharey=False, legend_out=True, hue='has_div')\n",
    "grid.map_dataframe(sns.countplot, x='is_correct')\n",
    "grid.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = sns.FacetGrid(component_df[(component_df.has_div == False) & (component_df.has_merge == False)], col='ds_name', col_wrap=4, sharex=True, sharey=False, legend_out=True)\n",
    "grid.map_dataframe(sns.countplot, x='is_correct')\n",
    "grid.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = sns.FacetGrid(component_df[(component_df.has_div == True) & (component_df.has_merge == False)], col='ds_name', col_wrap=4, sharex=True, sharey=False, legend_out=True)\n",
    "grid.map_dataframe(sns.countplot, x='is_correct')\n",
    "grid.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "grid = sns.FacetGrid(component_df, col='ds_name', col_wrap=4, sharex=False, sharey=False, legend_out=True, hue='has_merge')\n",
    "grid.map_dataframe(sns.histplot, x='total_edges')\n",
    "grid.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "grid = sns.FacetGrid(component_df[component_df['total_edges'] > 0], col='ds_name', col_wrap=4, sharex=False, sharey=False, legend_out=True, hue='has_merge')\n",
    "grid.map_dataframe(sns.scatterplot, x='total_edges', y='prop_correct')\n",
    "grid.add_legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What else do we want to know about merge vs. mergeless components\n",
    "\n",
    "- num divisions (correct/incorrect)\n",
    "- division asymmetry coloured by correct/incorrect merge/mergeless\n",
    "- angle of division coloured by correct/incorrect merge/mergeless\n",
    "- sum of cell area vs. parent area coloured by correct/incorrect\n",
    "- would be good to learn a migration measure from tracks with no divisions\n",
    "    - how many of these've we got?\n",
    "- distance from borders?\n",
    "- specifically looking for discriminators within the mergeless components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_info_df = pd.read_csv(os.path.join(SOL_ROOT, 'all_node_info.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_info_df['child_area_prop'] = np.where(all_info_df['child_area_sum'] > 0, all_info_df['child_area_sum'] / all_info_df['node_area'], -1)\n",
    "all_info_df['neighbour_rank_difference'] = np.where(all_info_df['second_chosen_neighbour_rank'] >= 0, all_info_df['second_chosen_neighbour_rank'] - all_info_df['first_chosen_neighbour_rank'], -1)\n",
    "all_info_df['neighbour_rank_sum'] = np.where(all_info_df['second_chosen_neighbour_rank'] >= 0, all_info_df['second_chosen_neighbour_rank'] + all_info_df['first_chosen_neighbour_rank'], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_info_df.to_csv(os.path.join(SOL_ROOT, 'all_node_info.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "grid = sns.FacetGrid(all_info_df[all_info_df.is_parent], col='ds_name', col_wrap=4, sharex=False, sharey=False, legend_out=True, hue='div_correct')\n",
    "grid.map_dataframe(sns.histplot, x='neighbour_rank_sum')\n",
    "grid.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_info_df[all_info_df.first_child_distance >= 0].value_counts('mig_correct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_info_df[all_info_df.second_child_distance >= 0].value_counts('div_correct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trexp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
